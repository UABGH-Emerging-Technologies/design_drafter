{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Documentation Key Features Chat-Based UML Revision Workflow: Generate, revise, and correct UML diagrams interactively via a conversational interface. See UML Rendering & Error Handling Workflow and Architecture Overview . Generic Error Handler: All errors are captured, reported, and auto-corrected when possible. See Error Handling . Environment Setup Copy the example file and set your LLM endpoint and key: cp .env.example .env Required env vars: UMLBOT_LLM_API_BASE UMLBOT_LLM_API_KEY Optional: UMLBOT_LLM_MODEL (defaults to gpt-4o-mini ) UMLBOT_PLANTUML_SERVER_URL_TEMPLATE (defaults to http://localhost:8080/png/{encoded} ) UMLBOT_CORS_ALLOW_ORIGINS Main Documentation Workflows : main workflows. Configuration : Configuration Details User Interface Architecture Overview Test Coverage If you found this helpful in your work, please cite: Godwin, R. C. , Melvin, R. L. , \u201cToward Efficient Data Science: A Comprehensive MLOps Template for Collaborative Code Development and Automation\u201d, SoftwareX, 26, 101723, 2024","title":"Home"},{"location":"index.html#documentation","text":"","title":"Documentation"},{"location":"index.html#key-features","text":"Chat-Based UML Revision Workflow: Generate, revise, and correct UML diagrams interactively via a conversational interface. See UML Rendering & Error Handling Workflow and Architecture Overview . Generic Error Handler: All errors are captured, reported, and auto-corrected when possible. See Error Handling .","title":"Key Features"},{"location":"index.html#environment-setup","text":"Copy the example file and set your LLM endpoint and key: cp .env.example .env Required env vars: UMLBOT_LLM_API_BASE UMLBOT_LLM_API_KEY Optional: UMLBOT_LLM_MODEL (defaults to gpt-4o-mini ) UMLBOT_PLANTUML_SERVER_URL_TEMPLATE (defaults to http://localhost:8080/png/{encoded} ) UMLBOT_CORS_ALLOW_ORIGINS","title":"Environment Setup"},{"location":"index.html#main-documentation","text":"Workflows : main workflows. Configuration : Configuration Details User Interface Architecture Overview Test Coverage If you found this helpful in your work, please cite: Godwin, R. C. , Melvin, R. L. , \u201cToward Efficient Data Science: A Comprehensive MLOps Template for Collaborative Code Development and Automation\u201d, SoftwareX, 26, 101723, 2024","title":"Main Documentation"},{"location":"UMLBot_streamlit_app.html","text":"UML Diagram Rendering & Chat-Based Revision Workflow This section describes the end-to-end workflow for rendering and revising UML diagrams in the Gradio-based UMLBot UI, including chat-based iterative refinement, PlantUML code generation, encoding, server communication, image rendering, and error handling. Chat-Based UML Revision & Error Correction Workflow Conversational Revision: Users interact with the system via a chat interface, describing the desired system, requesting changes, or asking for corrections in natural language. Iterative Updates: Each chat message triggers prompt construction, LLM invocation, and diagram/code update. Users can request additions, removals, or clarifications at any step. Error Handling & Auto-Correction: If an error occurs (e.g., invalid UML, rendering failure), the system: Surfaces a clear, actionable error message in the chat/status area. Attempts to auto-correct the issue using LLM-based repair strategies. Provides fallback PlantUML code or diagrams if correction fails. Transparency: All status updates, errors, and corrections are visible in the chat, ensuring users understand what went wrong and how it was addressed. 1. PlantUML Code Generation User Input: The user provides a free-text description and selects a UML diagram type (and optionally a theme) in the Gradio UI. Prompt Construction: The UMLDraftHandler loads a prompty template and fills in variables ( diagram_type , description , theme ). The template is validated for required placeholders. If the template or variables are invalid, a ValueError is raised. LLM Invocation: The handler invokes the configured LLM (e.g., OpenAI via LangChain) with the constructed prompt. On LLM errors (e.g., API failure, invalid response), a fallback PlantUML stub is generated and an error message is set for the UI. 2. PlantUML Code Encoding and Server Request Encoding: The generated PlantUML code is URL-encoded using urllib.parse.quote . Image URL Construction: The encoded string is inserted into a PlantUML server URL template (e.g., http://localhost:8080/svg/{encoded} ). Image Fetch: The app sends an HTTP GET request to the PlantUML server to retrieve the rendered diagram image. If the request fails (network error, timeout, non-200 response), the error is appended to the UI status message. 3. UI Rendering and Error Handling Image Rendering: If the image is successfully fetched, it is loaded as a PIL Image and displayed in the Gradio UI alongside the generated PlantUML code. Error Handling in Chat Workflow: LLM/Prompt Errors: The UI displays a fallback PlantUML stub and a status message indicating the LLM error in the chat/status area. PlantUML Server/Image Fetch Errors: The UI displays the PlantUML code, but the image preview is blank or missing. The status message includes details of the fetch failure in the chat/status area. Image Conversion Errors: If the image cannot be converted to a valid format, the UI shows a status message with the conversion error in the chat/status area. Auto-Correction: When possible, the system attempts to auto-correct errors and informs the user of the correction attempt and result. Status Messaging: All errors, corrections, and status updates are surfaced to the user via the chat and Markdown status area in the UI, ensuring transparency for debugging and user feedback. Summary Diagram sequenceDiagram participant User participant GradioUI participant UMLDraftHandler participant LLM participant PlantUMLServer User->>GradioUI: Enter description, select type GradioUI->>UMLDraftHandler: Build prompt, validate UMLDraftHandler->>LLM: Invoke with prompt LLM-->>UMLDraftHandler: PlantUML code (or error) UMLDraftHandler-->>GradioUI: Return code (or fallback) GradioUI->>PlantUMLServer: GET /plantuml/png/{encoded} PlantUMLServer-->>GradioUI: Image bytes (or error) GradioUI-->>User: Show code, image, status References app/gradio_app.py UMLBot/uml_draft_handler.py assets/uml_diagram.prompty","title":"User Interface"},{"location":"UMLBot_streamlit_app.html#uml-diagram-rendering-chat-based-revision-workflow","text":"This section describes the end-to-end workflow for rendering and revising UML diagrams in the Gradio-based UMLBot UI, including chat-based iterative refinement, PlantUML code generation, encoding, server communication, image rendering, and error handling.","title":"UML Diagram Rendering &amp; Chat-Based Revision Workflow"},{"location":"UMLBot_streamlit_app.html#chat-based-uml-revision-error-correction-workflow","text":"Conversational Revision: Users interact with the system via a chat interface, describing the desired system, requesting changes, or asking for corrections in natural language. Iterative Updates: Each chat message triggers prompt construction, LLM invocation, and diagram/code update. Users can request additions, removals, or clarifications at any step. Error Handling & Auto-Correction: If an error occurs (e.g., invalid UML, rendering failure), the system: Surfaces a clear, actionable error message in the chat/status area. Attempts to auto-correct the issue using LLM-based repair strategies. Provides fallback PlantUML code or diagrams if correction fails. Transparency: All status updates, errors, and corrections are visible in the chat, ensuring users understand what went wrong and how it was addressed.","title":"Chat-Based UML Revision &amp; Error Correction Workflow"},{"location":"UMLBot_streamlit_app.html#1-plantuml-code-generation","text":"User Input: The user provides a free-text description and selects a UML diagram type (and optionally a theme) in the Gradio UI. Prompt Construction: The UMLDraftHandler loads a prompty template and fills in variables ( diagram_type , description , theme ). The template is validated for required placeholders. If the template or variables are invalid, a ValueError is raised. LLM Invocation: The handler invokes the configured LLM (e.g., OpenAI via LangChain) with the constructed prompt. On LLM errors (e.g., API failure, invalid response), a fallback PlantUML stub is generated and an error message is set for the UI.","title":"1. PlantUML Code Generation"},{"location":"UMLBot_streamlit_app.html#2-plantuml-code-encoding-and-server-request","text":"Encoding: The generated PlantUML code is URL-encoded using urllib.parse.quote . Image URL Construction: The encoded string is inserted into a PlantUML server URL template (e.g., http://localhost:8080/svg/{encoded} ). Image Fetch: The app sends an HTTP GET request to the PlantUML server to retrieve the rendered diagram image. If the request fails (network error, timeout, non-200 response), the error is appended to the UI status message.","title":"2. PlantUML Code Encoding and Server Request"},{"location":"UMLBot_streamlit_app.html#3-ui-rendering-and-error-handling","text":"Image Rendering: If the image is successfully fetched, it is loaded as a PIL Image and displayed in the Gradio UI alongside the generated PlantUML code. Error Handling in Chat Workflow: LLM/Prompt Errors: The UI displays a fallback PlantUML stub and a status message indicating the LLM error in the chat/status area. PlantUML Server/Image Fetch Errors: The UI displays the PlantUML code, but the image preview is blank or missing. The status message includes details of the fetch failure in the chat/status area. Image Conversion Errors: If the image cannot be converted to a valid format, the UI shows a status message with the conversion error in the chat/status area. Auto-Correction: When possible, the system attempts to auto-correct errors and informs the user of the correction attempt and result. Status Messaging: All errors, corrections, and status updates are surfaced to the user via the chat and Markdown status area in the UI, ensuring transparency for debugging and user feedback.","title":"3. UI Rendering and Error Handling"},{"location":"UMLBot_streamlit_app.html#summary-diagram","text":"sequenceDiagram participant User participant GradioUI participant UMLDraftHandler participant LLM participant PlantUMLServer User->>GradioUI: Enter description, select type GradioUI->>UMLDraftHandler: Build prompt, validate UMLDraftHandler->>LLM: Invoke with prompt LLM-->>UMLDraftHandler: PlantUML code (or error) UMLDraftHandler-->>GradioUI: Return code (or fallback) GradioUI->>PlantUMLServer: GET /plantuml/png/{encoded} PlantUMLServer-->>GradioUI: Image bytes (or error) GradioUI-->>User: Show code, image, status","title":"Summary Diagram"},{"location":"UMLBot_streamlit_app.html#references","text":"app/gradio_app.py UMLBot/uml_draft_handler.py assets/uml_diagram.prompty","title":"References"},{"location":"architecture.html","text":"System Component to Code Module and LLM Utility Mapping UI Layer - Gradio UI: Handles user input, diagram type selection, preview, and download. - Code: GradioInterface (to be implemented in the main app, e.g., UMLBot_streamlit_app.md or a new UI module) - Interacts with: ChatBotService (service layer) Service Layer - ChatBotService: Orchestrates the flow from user request to diagram generation. - Code: New module/class, e.g., UMLBot/service/chatbot_service.py - Uses: NLPParser , DiagramBuilder - NLPParser: Converts free text to UML constructs using LLM. - Code: New module/class, e.g., UMLBot/service/nlp_parser.py - Leverages: llm_utils/aiweb_common/generate/ChatServicer.py , PromptAssembler.py for LLM calls and prompt construction. - DiagramBuilder: Builds PlantUML code from parsed model, manages validation and repair. - Code: New module/class, e.g., UMLBot/service/diagram_builder.py - Uses: PlantUMLValidator , ThemeManager , PlantUmlClient - PlantUMLValidator & AutoRepairStrategy: Validate and repair PlantUML code. - Code: New module/class, e.g., UMLBot/service/plantuml_validator.py - Can leverage: LLM utilities for advanced repair (e.g., ChatServicer for LLM-based repair suggestions) - ThemeManager: Supplies PlantUML themes. - Code: New module/class, e.g., UMLBot/service/theme_manager.py Infrastructure Layer - PlantUmlClient: Renders PlantUML code (remote or local). - Code: New module/class, e.g., UMLBot/infrastructure/plantuml_client.py - Implements: HTTP and local JAR rendering, with failover logic. - Database, Auth, Monitoring: Standard integrations for user/session/history, authentication, and logging. - Code: To be implemented as needed, e.g., UMLBot/infrastructure/db.py , auth.py , monitoring.py Supporting Services - DownloadController: Serves images to the UI. - Code: Part of UI or a small API endpoint. Chat-Based UML Revision Workflow The chat workflow enables users to iteratively describe, revise, and correct UML diagrams through conversational interaction. The UI (Gradio/Streamlit) relays user messages to the service layer, which orchestrates prompt construction, LLM invocation, and diagram updates. Each chat turn can request new features, corrections, or explanations; the system updates the PlantUML code and diagram accordingly. The workflow supports error correction by leveraging LLM-based auto-repair and surfacing actionable feedback to the user. Generic Error Handler Integration The generic error handler is integrated across all layers: UI: Displays status and error messages, provides fallback diagrams. Service: Captures errors from LLM, prompt, and rendering steps; attempts auto-correction. Infrastructure: Handles network and rendering failures, reporting issues to the service layer. All errors are routed through the error handler, ensuring transparency and robust recovery. LLM Utility Integration - All LLM calls (NLP parsing, auto-repair, chat-based revision, error handling) should use the llm_utils/aiweb_common/generate/ classes: - ChatServicer for conversational LLM calls. - PromptAssembler for prompt construction. - QueryInterface for abstraction. - ObjectFactory for extensibility (e.g., plugin new diagram types). - WorkflowHandler for orchestrating multi-step flows and logging. - GenericErrorHandler for capturing, reporting, and auto-correcting errors. This mapping ensures that all LLM-related logic and error handling are centralized and reusable, following the project's coding standards and leveraging the common codebase. Next steps: Specify in detail how and where to leverage these LLM interface classes and error handler in the design.","title":"Architecture"},{"location":"UMLBot/__init__.html","text":"UMLBot core package.","title":"Package Init"},{"location":"UMLBot/api_server.html","text":"FastAPI app factory for the UMLBot HTTP API. create_api_app ( generate_fn = None ) Builds the FastAPI application exposing JSON endpoints for diagram generation. Source code in UMLBot/api_server.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def create_api_app ( generate_fn : Callable [[ str , str , str | None ], \"DiagramGenerationResult\" ] | None = None , ) -> FastAPI : \"\"\"Builds the FastAPI application exposing JSON endpoints for diagram generation.\"\"\" if generate_fn is None : generate_fn = generate_diagram_from_description api_app = FastAPI ( title = \"UMLBot HTTP API\" ) api_app . add_middleware ( CORSMiddleware , allow_origins = UMLBotConfig . CORS_ALLOW_ORIGINS , allow_methods = [ \"POST\" , \"OPTIONS\" ], allow_headers = [ \"Content-Type\" , \"Authorization\" , \"Accept\" ], ) @api_app . post ( \"/api/generate\" ) async def generate_endpoint ( request : Request ): \"\"\"Handle diagram generation requests from the frontend.\"\"\" try : data = await request . json () description = data . get ( \"description\" ) diagram_type = data . get ( \"diagram_type\" ) theme = data . get ( \"theme\" ) if not description or not diagram_type : return JSONResponse ( status_code = 400 , content = { \"status\" : \"error\" , \"message\" : \"Missing required fields: description, diagram_type\" , }, ) result = generate_fn ( description , diagram_type , theme ) if not result . plantuml_code : return JSONResponse ( status_code = 500 , content = { \"status\" : \"error\" , \"message\" : result . status_message or \"Generation failed\" , }, ) image_base64 = diagram_image_to_base64 ( result . pil_image ) return { \"status\" : \"ok\" , \"plantuml_code\" : result . plantuml_code , \"image_base64\" : image_base64 , \"image_url\" : result . image_url , \"message\" : result . status_message , } except Exception : logging . exception ( \"Unhandled exception in /api/generate\" ) return JSONResponse ( status_code = 500 , content = { \"status\" : \"error\" , \"message\" : \"Internal server error\" }, ) return api_app","title":"API Server"},{"location":"UMLBot/api_server.html#UMLBot.api_server.create_api_app","text":"Builds the FastAPI application exposing JSON endpoints for diagram generation. Source code in UMLBot/api_server.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def create_api_app ( generate_fn : Callable [[ str , str , str | None ], \"DiagramGenerationResult\" ] | None = None , ) -> FastAPI : \"\"\"Builds the FastAPI application exposing JSON endpoints for diagram generation.\"\"\" if generate_fn is None : generate_fn = generate_diagram_from_description api_app = FastAPI ( title = \"UMLBot HTTP API\" ) api_app . add_middleware ( CORSMiddleware , allow_origins = UMLBotConfig . CORS_ALLOW_ORIGINS , allow_methods = [ \"POST\" , \"OPTIONS\" ], allow_headers = [ \"Content-Type\" , \"Authorization\" , \"Accept\" ], ) @api_app . post ( \"/api/generate\" ) async def generate_endpoint ( request : Request ): \"\"\"Handle diagram generation requests from the frontend.\"\"\" try : data = await request . json () description = data . get ( \"description\" ) diagram_type = data . get ( \"diagram_type\" ) theme = data . get ( \"theme\" ) if not description or not diagram_type : return JSONResponse ( status_code = 400 , content = { \"status\" : \"error\" , \"message\" : \"Missing required fields: description, diagram_type\" , }, ) result = generate_fn ( description , diagram_type , theme ) if not result . plantuml_code : return JSONResponse ( status_code = 500 , content = { \"status\" : \"error\" , \"message\" : result . status_message or \"Generation failed\" , }, ) image_base64 = diagram_image_to_base64 ( result . pil_image ) return { \"status\" : \"ok\" , \"plantuml_code\" : result . plantuml_code , \"image_base64\" : image_base64 , \"image_url\" : result . image_url , \"message\" : result . status_message , } except Exception : logging . exception ( \"Unhandled exception in /api/generate\" ) return JSONResponse ( status_code = 500 , content = { \"status\" : \"error\" , \"message\" : \"Internal server error\" }, ) return api_app","title":"create_api_app"},{"location":"UMLBot/exceptions.html","text":"Custom exceptions for UMLBot modules. LLMError Bases: Exception Raised when an LLM invocation fails. Source code in UMLBot/exceptions.py 3 4 class LLMError ( Exception ): \"\"\"Raised when an LLM invocation fails.\"\"\"","title":"Exceptions"},{"location":"UMLBot/exceptions.html#UMLBot.exceptions.LLMError","text":"Bases: Exception Raised when an LLM invocation fails. Source code in UMLBot/exceptions.py 3 4 class LLMError ( Exception ): \"\"\"Raised when an LLM invocation fails.\"\"\"","title":"LLMError"},{"location":"UMLBot/llm_interface.html","text":"Lightweight LLM adapter abstractions used in tests. LLMCallable Bases: Protocol Callable protocol for LLM backends used by the adapter. Source code in UMLBot/llm_interface.py 11 12 13 14 15 class LLMCallable ( Protocol ): \"\"\"Callable protocol for LLM backends used by the adapter.\"\"\" def __call__ ( self , prompt : str ) -> str : # pragma: no cover - Protocol signature only ... LLMInterface Bases: Protocol Minimal interface for LLM invocation used in the codebase and tests. Source code in UMLBot/llm_interface.py 18 19 20 21 22 23 24 25 class LLMInterface ( Protocol ): \"\"\"Minimal interface for LLM invocation used in the codebase and tests.\"\"\" def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the LLM synchronously.\"\"\" async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the LLM asynchronously.\"\"\" invoke ( prompt ) Invoke the LLM synchronously. Source code in UMLBot/llm_interface.py 21 22 def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the LLM synchronously.\"\"\" invoke_async ( prompt ) async Invoke the LLM asynchronously. Source code in UMLBot/llm_interface.py 24 25 async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the LLM asynchronously.\"\"\" LangchainLLMAdapter Adapter that normalizes a callable into an LLM interface. Source code in UMLBot/llm_interface.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class LangchainLLMAdapter : \"\"\"Adapter that normalizes a callable into an LLM interface.\"\"\" def __init__ ( self , config : dict [ str , Any ]) -> None : \"\"\"Initialize the adapter with a callable under ``llm_callable``.\"\"\" llm_callable = config . get ( \"llm_callable\" ) if llm_callable is None or not callable ( llm_callable ): raise ValueError ( \"config['llm_callable'] must be a callable.\" ) self . _llm_callable : LLMCallable = llm_callable def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the underlying callable and normalize errors.\"\"\" try : return self . _llm_callable ( prompt ) except Exception as exc : # pragma: no cover - exception path exercised in tests raise LLMError ( str ( exc )) from exc async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the callable asynchronously using a thread helper.\"\"\" try : return await asyncio . to_thread ( self . _llm_callable , prompt ) except Exception as exc : raise LLMError ( str ( exc )) from exc __init__ ( config ) Initialize the adapter with a callable under llm_callable . Source code in UMLBot/llm_interface.py 31 32 33 34 35 36 def __init__ ( self , config : dict [ str , Any ]) -> None : \"\"\"Initialize the adapter with a callable under ``llm_callable``.\"\"\" llm_callable = config . get ( \"llm_callable\" ) if llm_callable is None or not callable ( llm_callable ): raise ValueError ( \"config['llm_callable'] must be a callable.\" ) self . _llm_callable : LLMCallable = llm_callable invoke ( prompt ) Invoke the underlying callable and normalize errors. Source code in UMLBot/llm_interface.py 38 39 40 41 42 43 def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the underlying callable and normalize errors.\"\"\" try : return self . _llm_callable ( prompt ) except Exception as exc : # pragma: no cover - exception path exercised in tests raise LLMError ( str ( exc )) from exc invoke_async ( prompt ) async Invoke the callable asynchronously using a thread helper. Source code in UMLBot/llm_interface.py 45 46 47 48 49 50 async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the callable asynchronously using a thread helper.\"\"\" try : return await asyncio . to_thread ( self . _llm_callable , prompt ) except Exception as exc : raise LLMError ( str ( exc )) from exc","title":"LLM Interface"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LLMCallable","text":"Bases: Protocol Callable protocol for LLM backends used by the adapter. Source code in UMLBot/llm_interface.py 11 12 13 14 15 class LLMCallable ( Protocol ): \"\"\"Callable protocol for LLM backends used by the adapter.\"\"\" def __call__ ( self , prompt : str ) -> str : # pragma: no cover - Protocol signature only ...","title":"LLMCallable"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LLMInterface","text":"Bases: Protocol Minimal interface for LLM invocation used in the codebase and tests. Source code in UMLBot/llm_interface.py 18 19 20 21 22 23 24 25 class LLMInterface ( Protocol ): \"\"\"Minimal interface for LLM invocation used in the codebase and tests.\"\"\" def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the LLM synchronously.\"\"\" async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the LLM asynchronously.\"\"\"","title":"LLMInterface"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LLMInterface.invoke","text":"Invoke the LLM synchronously. Source code in UMLBot/llm_interface.py 21 22 def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the LLM synchronously.\"\"\"","title":"invoke"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LLMInterface.invoke_async","text":"Invoke the LLM asynchronously. Source code in UMLBot/llm_interface.py 24 25 async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the LLM asynchronously.\"\"\"","title":"invoke_async"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LangchainLLMAdapter","text":"Adapter that normalizes a callable into an LLM interface. Source code in UMLBot/llm_interface.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class LangchainLLMAdapter : \"\"\"Adapter that normalizes a callable into an LLM interface.\"\"\" def __init__ ( self , config : dict [ str , Any ]) -> None : \"\"\"Initialize the adapter with a callable under ``llm_callable``.\"\"\" llm_callable = config . get ( \"llm_callable\" ) if llm_callable is None or not callable ( llm_callable ): raise ValueError ( \"config['llm_callable'] must be a callable.\" ) self . _llm_callable : LLMCallable = llm_callable def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the underlying callable and normalize errors.\"\"\" try : return self . _llm_callable ( prompt ) except Exception as exc : # pragma: no cover - exception path exercised in tests raise LLMError ( str ( exc )) from exc async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the callable asynchronously using a thread helper.\"\"\" try : return await asyncio . to_thread ( self . _llm_callable , prompt ) except Exception as exc : raise LLMError ( str ( exc )) from exc","title":"LangchainLLMAdapter"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LangchainLLMAdapter.__init__","text":"Initialize the adapter with a callable under llm_callable . Source code in UMLBot/llm_interface.py 31 32 33 34 35 36 def __init__ ( self , config : dict [ str , Any ]) -> None : \"\"\"Initialize the adapter with a callable under ``llm_callable``.\"\"\" llm_callable = config . get ( \"llm_callable\" ) if llm_callable is None or not callable ( llm_callable ): raise ValueError ( \"config['llm_callable'] must be a callable.\" ) self . _llm_callable : LLMCallable = llm_callable","title":"__init__"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LangchainLLMAdapter.invoke","text":"Invoke the underlying callable and normalize errors. Source code in UMLBot/llm_interface.py 38 39 40 41 42 43 def invoke ( self , prompt : str ) -> str : \"\"\"Invoke the underlying callable and normalize errors.\"\"\" try : return self . _llm_callable ( prompt ) except Exception as exc : # pragma: no cover - exception path exercised in tests raise LLMError ( str ( exc )) from exc","title":"invoke"},{"location":"UMLBot/llm_interface.html#UMLBot.llm_interface.LangchainLLMAdapter.invoke_async","text":"Invoke the callable asynchronously using a thread helper. Source code in UMLBot/llm_interface.py 45 46 47 48 49 50 async def invoke_async ( self , prompt : str ) -> str : \"\"\"Invoke the callable asynchronously using a thread helper.\"\"\" try : return await asyncio . to_thread ( self . _llm_callable , prompt ) except Exception as exc : raise LLMError ( str ( exc )) from exc","title":"invoke_async"},{"location":"UMLBot/uml_draft_handler.html","text":"UMLDraftHandler: Handles UML diagram generation via LLM using prompty template. Refactored to inherit WorkflowHandler and provide methods for: - Loading prompty file - Constructing prompt with required variables - Generating diagram (LLM call, error handling) No UI/Gradio code. UMLDraftHandler Bases: WorkflowHandler Handler for generating UML diagrams using an LLM and a prompty template. Attributes: prompty_path ( Path ) \u2013 Path to the prompty file. Source code in UMLBot/uml_draft_handler.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class UMLDraftHandler ( WorkflowHandler ): \"\"\" Handler for generating UML diagrams using an LLM and a prompty template. Attributes: prompty_path (Path): Path to the prompty file. \"\"\" def __init__ ( self , config : Optional [ UMLBotConfig ] = None ): \"\"\"Initialize the handler with optional configuration overrides.\"\"\" super () . __init__ () self . prompty_path = ( Path ( __file__ ) . resolve () . parent . parent / \"assets\" / \"uml_diagram.prompty\" ) self . config = config or UMLBotConfig () def _validate_prompt_template ( self , template : str ) -> None : \"\"\" Validates a UML prompt template for required placeholders and syntax. This validator fully separates Python `.format()` style (`{name}`) and Jinja2 style (`{{ name }}` or `{% block %}`) placeholders. It does not flag valid Jinja2 constructs as malformed Python placeholders. Args: template (str): The prompt template string to validate. Raises: ValueError: If required placeholders are missing, curly braces/tags are unbalanced, or placeholders/tags are malformed. Examples: Python `.format()` style: \"Generate a {diagram_type} diagram for: {description} {theme}\" Jinja2 style: \"Generate a {{ diagram_type }} diagram for: {{ description }} {% if theme %}Theme: {{ theme }}{% endif %}\" \"\"\" import re # Remove all Jinja2 tags for Python-style validation jinja2_tag_pattern = r \"(\\{\\{.*?\\}\\}|\\{%.*?%\\})\" template_no_jinja2 = re . sub ( jinja2_tag_pattern , \"\" , template ) # 1. Python-style placeholder validation (only on non-Jinja2 content) py_placeholder_pattern = r \"\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}\" py_placeholders = set ( re . findall ( py_placeholder_pattern , template_no_jinja2 )) # Malformed Python-style: empty {}, non-identifier, or unclosed malformed_py = [] if re . search ( r \"\\{\\s*\\}\" , template_no_jinja2 ): malformed_py . append ( \"Empty Python-style placeholder\" ) if re . search ( r \"\\{[^a-zA-Z_][^}]*\\}\" , template_no_jinja2 ): malformed_py . append ( \"Malformed Python-style placeholder\" ) if template_no_jinja2 . count ( \"{\" ) != template_no_jinja2 . count ( \"}\" ): malformed_py . append ( \"Unbalanced curly braces in Python-style section\" ) # 2. Jinja2-style placeholder validation (only on Jinja2 tags) jinja2_var_pattern = r \"\\{\\{\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\}\\}\" jinja2_placeholders = set ( re . findall ( jinja2_var_pattern , template )) jinja2_block_pattern = r \"\\{%\\s*([a-zA-Z_][a-zA-Z0-9_ ]*)\\s*%\\}\" jinja2_blocks = re . findall ( jinja2_block_pattern , template ) # Malformed Jinja2: unclosed or incomplete tags malformed_jinja2 = [] if re . search ( r \"\\{\\{[^\\}]*$\" , template ) or re . search ( r \"\\{%[^\\}]*$\" , template ): malformed_jinja2 . append ( \"Unclosed Jinja2 tag\" ) if template . count ( \"{{\" ) != template . count ( \"}}\" ): malformed_jinja2 . append ( \"Unbalanced Jinja2 variable tags\" ) if template . count ( \"{%\" ) != template . count ( \"%}\" ): malformed_jinja2 . append ( \"Unbalanced Jinja2 block tags\" ) if malformed_py or malformed_jinja2 : raise ValueError ( f \"Malformed placeholder(s) found: { malformed_jinja2 + malformed_py } \" ) # 3. Required placeholders (must be present in either style) all_placeholders = py_placeholders | jinja2_placeholders required = { \"diagram_type\" , \"description\" } missing = required - all_placeholders if missing : raise ValueError ( f \"Missing required placeholder(s): { ', ' . join ( sorted ( missing )) } . \" \"Required placeholders must be present in either Python `.format()` ( {name} ) or Jinja2 ({{ name }}) style.\" ) # 4. If 'theme' is referenced, ensure it's a valid placeholder in either style theme_referenced = ( \"theme\" in template or re . search ( r \"\\{\\{\\s*theme\\s*\\}\\}\" , template ) or re . search ( r \"\\{theme\\}\" , template ) ) theme_present = \"theme\" in all_placeholders if theme_referenced and not theme_present : raise ValueError ( \"Template references 'theme' but does not use a valid ' {theme} ' or '{{ theme }}' placeholder.\" ) # All checks passed def construct_prompt ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , ) -> Any : \"\"\" Loads the prompty template and constructs the prompt with provided variables. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. Returns: ChatPromptTemplate: Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError: If prompty file is missing. ValueError: If prompty file is invalid. \"\"\" prompt_template = self . load_prompty () # Ensure prompt_template is a ChatPromptTemplate and validate its template # Skipping prompt template validation for now: # if hasattr(self, \"_validate_prompt_template\"): # self._validate_prompt_template(prompt_template.template) variables = { \"diagram_type\" : escape_curly_braces ( diagram_type ), \"description\" : escape_curly_braces ( description ), \"theme\" : escape_curly_braces ( theme ), } return prompt_template . format_prompt ( ** variables ) def process ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , llm_interface : Optional [ Any ] = None , retry_manager : Optional [ \"UMLRetryManager\" ] = None , ) -> str : \"\"\" Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. llm_interface: Optional LLM interface for invocation (must have .invoke()). retry_manager (Optional[UMLRetryManager]): Helper for tracking retries and error context. Returns: str: PlantUML diagram code. Raises: RuntimeError: If all retries fail, with error context. \"\"\" if retry_manager is None : retry_manager = UMLRetryManager ( max_retries = 3 ) while retry_manager . should_retry (): try : prompt = self . construct_prompt ( diagram_type , description , theme ) if llm_interface is None : raise ValueError ( \"LLM interface must be provided for diagram generation.\" ) # Extract messages if present (Langchain ChatPromptValue) if hasattr ( prompt , \"to_messages\" ): messages = prompt . to_messages () if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif hasattr ( prompt , \"messages\" ): messages = prompt . messages if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif isinstance ( prompt , str ): prompt_input = prompt else : raise TypeError ( \"Prompt is not a recognized type for LLM input.\" ) response = llm_interface . invoke ( prompt_input ) diagram_code = self . check_content_type ( response ) return diagram_code except Exception as exc : retry_manager . record_error ( exc ) logging . exception ( \"UML diagram generation failed (attempt %s )\" , retry_manager . attempt ) # After max retries, raise with error context error_msg = ( f \"UML diagram generation failed after { retry_manager . max_retries } attempts. \\n \" f \"Error context: \\n { retry_manager . error_context () } \" ) raise RuntimeError ( error_msg ) __init__ ( config = None ) Initialize the handler with optional configuration overrides. Source code in UMLBot/uml_draft_handler.py 76 77 78 79 80 81 82 def __init__ ( self , config : Optional [ UMLBotConfig ] = None ): \"\"\"Initialize the handler with optional configuration overrides.\"\"\" super () . __init__ () self . prompty_path = ( Path ( __file__ ) . resolve () . parent . parent / \"assets\" / \"uml_diagram.prompty\" ) self . config = config or UMLBotConfig () construct_prompt ( diagram_type , description , theme = None ) Loads the prompty template and constructs the prompt with provided variables. Parameters: diagram_type ( str ) \u2013 Type of UML diagram to generate. description ( str ) \u2013 Description of the system/process to diagram. theme ( Optional [ str ] , default: None ) \u2013 PlantUML theme/style. Returns: ChatPromptTemplate ( Any ) \u2013 Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError \u2013 If prompty file is missing. ValueError \u2013 If prompty file is invalid. Source code in UMLBot/uml_draft_handler.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def construct_prompt ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , ) -> Any : \"\"\" Loads the prompty template and constructs the prompt with provided variables. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. Returns: ChatPromptTemplate: Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError: If prompty file is missing. ValueError: If prompty file is invalid. \"\"\" prompt_template = self . load_prompty () # Ensure prompt_template is a ChatPromptTemplate and validate its template # Skipping prompt template validation for now: # if hasattr(self, \"_validate_prompt_template\"): # self._validate_prompt_template(prompt_template.template) variables = { \"diagram_type\" : escape_curly_braces ( diagram_type ), \"description\" : escape_curly_braces ( description ), \"theme\" : escape_curly_braces ( theme ), } return prompt_template . format_prompt ( ** variables ) process ( diagram_type , description , theme = None , llm_interface = None , retry_manager = None ) Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Parameters: diagram_type ( str ) \u2013 Type of UML diagram to generate. description ( str ) \u2013 Description of the system/process to diagram. theme ( Optional [ str ] , default: None ) \u2013 PlantUML theme/style. llm_interface ( Optional [ Any ] , default: None ) \u2013 Optional LLM interface for invocation (must have .invoke()). retry_manager ( Optional [ UMLRetryManager ] , default: None ) \u2013 Helper for tracking retries and error context. Returns: str ( str ) \u2013 PlantUML diagram code. Raises: RuntimeError \u2013 If all retries fail, with error context. Source code in UMLBot/uml_draft_handler.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def process ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , llm_interface : Optional [ Any ] = None , retry_manager : Optional [ \"UMLRetryManager\" ] = None , ) -> str : \"\"\" Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. llm_interface: Optional LLM interface for invocation (must have .invoke()). retry_manager (Optional[UMLRetryManager]): Helper for tracking retries and error context. Returns: str: PlantUML diagram code. Raises: RuntimeError: If all retries fail, with error context. \"\"\" if retry_manager is None : retry_manager = UMLRetryManager ( max_retries = 3 ) while retry_manager . should_retry (): try : prompt = self . construct_prompt ( diagram_type , description , theme ) if llm_interface is None : raise ValueError ( \"LLM interface must be provided for diagram generation.\" ) # Extract messages if present (Langchain ChatPromptValue) if hasattr ( prompt , \"to_messages\" ): messages = prompt . to_messages () if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif hasattr ( prompt , \"messages\" ): messages = prompt . messages if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif isinstance ( prompt , str ): prompt_input = prompt else : raise TypeError ( \"Prompt is not a recognized type for LLM input.\" ) response = llm_interface . invoke ( prompt_input ) diagram_code = self . check_content_type ( response ) return diagram_code except Exception as exc : retry_manager . record_error ( exc ) logging . exception ( \"UML diagram generation failed (attempt %s )\" , retry_manager . attempt ) # After max retries, raise with error context error_msg = ( f \"UML diagram generation failed after { retry_manager . max_retries } attempts. \\n \" f \"Error context: \\n { retry_manager . error_context () } \" ) raise RuntimeError ( error_msg ) UMLRetryManager Tracks retry count and error context for UML rendering attempts. Attributes: max_retries ( int ) \u2013 Maximum number of retries allowed. attempt ( int ) \u2013 Current attempt number. errors ( list [ str ] ) \u2013 List of error messages from each failed attempt. Source code in UMLBot/uml_draft_handler.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class UMLRetryManager : \"\"\" Tracks retry count and error context for UML rendering attempts. Attributes: max_retries (int): Maximum number of retries allowed. attempt (int): Current attempt number. errors (list[str]): List of error messages from each failed attempt. \"\"\" def __init__ ( self , max_retries : int = 3 ) -> None : \"\"\"Initialize the retry manager with a maximum retry count.\"\"\" self . max_retries = max_retries self . attempt = 0 self . errors : list [ str ] = [] def record_error ( self , error : Exception ) -> None : \"\"\"Record an error and increment the attempt counter.\"\"\" self . attempt += 1 self . errors . append ( str ( error )) def should_retry ( self ) -> bool : \"\"\"Return True when another retry attempt is allowed.\"\"\" return self . attempt < self . max_retries def last_error ( self ) -> str : \"\"\"Return the most recent error message, or empty string if none.\"\"\" return self . errors [ - 1 ] if self . errors else \"\" def error_context ( self ) -> str : \"\"\"Return a multi-line summary of attempts and errors.\"\"\" return \" \\n \" . join ( f \"Attempt { i + 1 } : { msg } \" for i , msg in enumerate ( self . errors )) __init__ ( max_retries = 3 ) Initialize the retry manager with a maximum retry count. Source code in UMLBot/uml_draft_handler.py 22 23 24 25 26 def __init__ ( self , max_retries : int = 3 ) -> None : \"\"\"Initialize the retry manager with a maximum retry count.\"\"\" self . max_retries = max_retries self . attempt = 0 self . errors : list [ str ] = [] error_context () Return a multi-line summary of attempts and errors. Source code in UMLBot/uml_draft_handler.py 41 42 43 def error_context ( self ) -> str : \"\"\"Return a multi-line summary of attempts and errors.\"\"\" return \" \\n \" . join ( f \"Attempt { i + 1 } : { msg } \" for i , msg in enumerate ( self . errors )) last_error () Return the most recent error message, or empty string if none. Source code in UMLBot/uml_draft_handler.py 37 38 39 def last_error ( self ) -> str : \"\"\"Return the most recent error message, or empty string if none.\"\"\" return self . errors [ - 1 ] if self . errors else \"\" record_error ( error ) Record an error and increment the attempt counter. Source code in UMLBot/uml_draft_handler.py 28 29 30 31 def record_error ( self , error : Exception ) -> None : \"\"\"Record an error and increment the attempt counter.\"\"\" self . attempt += 1 self . errors . append ( str ( error )) should_retry () Return True when another retry attempt is allowed. Source code in UMLBot/uml_draft_handler.py 33 34 35 def should_retry ( self ) -> bool : \"\"\"Return True when another retry attempt is allowed.\"\"\" return self . attempt < self . max_retries escape_curly_braces ( val ) Escapes all curly braces in a string for safe prompt injection. Args: val (Optional[str]): Input string. Returns: Optional[str]: String with '{' replaced by '{{' and '}' replaced by '}}'. Source code in UMLBot/uml_draft_handler.py 51 52 53 54 55 56 57 58 59 60 61 def escape_curly_braces ( val : Optional [ str ]) -> Optional [ str ]: \"\"\" Escapes all curly braces in a string for safe prompt injection. Args: val (Optional[str]): Input string. Returns: Optional[str]: String with '{' replaced by '{{' and '}' replaced by '}}'. \"\"\" if val is None : return val return val . replace ( \"{\" , \"{{\" ) . replace ( \"}\" , \"}}\" )","title":"UML Draft Handler"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLDraftHandler","text":"Bases: WorkflowHandler Handler for generating UML diagrams using an LLM and a prompty template. Attributes: prompty_path ( Path ) \u2013 Path to the prompty file. Source code in UMLBot/uml_draft_handler.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class UMLDraftHandler ( WorkflowHandler ): \"\"\" Handler for generating UML diagrams using an LLM and a prompty template. Attributes: prompty_path (Path): Path to the prompty file. \"\"\" def __init__ ( self , config : Optional [ UMLBotConfig ] = None ): \"\"\"Initialize the handler with optional configuration overrides.\"\"\" super () . __init__ () self . prompty_path = ( Path ( __file__ ) . resolve () . parent . parent / \"assets\" / \"uml_diagram.prompty\" ) self . config = config or UMLBotConfig () def _validate_prompt_template ( self , template : str ) -> None : \"\"\" Validates a UML prompt template for required placeholders and syntax. This validator fully separates Python `.format()` style (`{name}`) and Jinja2 style (`{{ name }}` or `{% block %}`) placeholders. It does not flag valid Jinja2 constructs as malformed Python placeholders. Args: template (str): The prompt template string to validate. Raises: ValueError: If required placeholders are missing, curly braces/tags are unbalanced, or placeholders/tags are malformed. Examples: Python `.format()` style: \"Generate a {diagram_type} diagram for: {description} {theme}\" Jinja2 style: \"Generate a {{ diagram_type }} diagram for: {{ description }} {% if theme %}Theme: {{ theme }}{% endif %}\" \"\"\" import re # Remove all Jinja2 tags for Python-style validation jinja2_tag_pattern = r \"(\\{\\{.*?\\}\\}|\\{%.*?%\\})\" template_no_jinja2 = re . sub ( jinja2_tag_pattern , \"\" , template ) # 1. Python-style placeholder validation (only on non-Jinja2 content) py_placeholder_pattern = r \"\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}\" py_placeholders = set ( re . findall ( py_placeholder_pattern , template_no_jinja2 )) # Malformed Python-style: empty {}, non-identifier, or unclosed malformed_py = [] if re . search ( r \"\\{\\s*\\}\" , template_no_jinja2 ): malformed_py . append ( \"Empty Python-style placeholder\" ) if re . search ( r \"\\{[^a-zA-Z_][^}]*\\}\" , template_no_jinja2 ): malformed_py . append ( \"Malformed Python-style placeholder\" ) if template_no_jinja2 . count ( \"{\" ) != template_no_jinja2 . count ( \"}\" ): malformed_py . append ( \"Unbalanced curly braces in Python-style section\" ) # 2. Jinja2-style placeholder validation (only on Jinja2 tags) jinja2_var_pattern = r \"\\{\\{\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\}\\}\" jinja2_placeholders = set ( re . findall ( jinja2_var_pattern , template )) jinja2_block_pattern = r \"\\{%\\s*([a-zA-Z_][a-zA-Z0-9_ ]*)\\s*%\\}\" jinja2_blocks = re . findall ( jinja2_block_pattern , template ) # Malformed Jinja2: unclosed or incomplete tags malformed_jinja2 = [] if re . search ( r \"\\{\\{[^\\}]*$\" , template ) or re . search ( r \"\\{%[^\\}]*$\" , template ): malformed_jinja2 . append ( \"Unclosed Jinja2 tag\" ) if template . count ( \"{{\" ) != template . count ( \"}}\" ): malformed_jinja2 . append ( \"Unbalanced Jinja2 variable tags\" ) if template . count ( \"{%\" ) != template . count ( \"%}\" ): malformed_jinja2 . append ( \"Unbalanced Jinja2 block tags\" ) if malformed_py or malformed_jinja2 : raise ValueError ( f \"Malformed placeholder(s) found: { malformed_jinja2 + malformed_py } \" ) # 3. Required placeholders (must be present in either style) all_placeholders = py_placeholders | jinja2_placeholders required = { \"diagram_type\" , \"description\" } missing = required - all_placeholders if missing : raise ValueError ( f \"Missing required placeholder(s): { ', ' . join ( sorted ( missing )) } . \" \"Required placeholders must be present in either Python `.format()` ( {name} ) or Jinja2 ({{ name }}) style.\" ) # 4. If 'theme' is referenced, ensure it's a valid placeholder in either style theme_referenced = ( \"theme\" in template or re . search ( r \"\\{\\{\\s*theme\\s*\\}\\}\" , template ) or re . search ( r \"\\{theme\\}\" , template ) ) theme_present = \"theme\" in all_placeholders if theme_referenced and not theme_present : raise ValueError ( \"Template references 'theme' but does not use a valid ' {theme} ' or '{{ theme }}' placeholder.\" ) # All checks passed def construct_prompt ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , ) -> Any : \"\"\" Loads the prompty template and constructs the prompt with provided variables. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. Returns: ChatPromptTemplate: Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError: If prompty file is missing. ValueError: If prompty file is invalid. \"\"\" prompt_template = self . load_prompty () # Ensure prompt_template is a ChatPromptTemplate and validate its template # Skipping prompt template validation for now: # if hasattr(self, \"_validate_prompt_template\"): # self._validate_prompt_template(prompt_template.template) variables = { \"diagram_type\" : escape_curly_braces ( diagram_type ), \"description\" : escape_curly_braces ( description ), \"theme\" : escape_curly_braces ( theme ), } return prompt_template . format_prompt ( ** variables ) def process ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , llm_interface : Optional [ Any ] = None , retry_manager : Optional [ \"UMLRetryManager\" ] = None , ) -> str : \"\"\" Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. llm_interface: Optional LLM interface for invocation (must have .invoke()). retry_manager (Optional[UMLRetryManager]): Helper for tracking retries and error context. Returns: str: PlantUML diagram code. Raises: RuntimeError: If all retries fail, with error context. \"\"\" if retry_manager is None : retry_manager = UMLRetryManager ( max_retries = 3 ) while retry_manager . should_retry (): try : prompt = self . construct_prompt ( diagram_type , description , theme ) if llm_interface is None : raise ValueError ( \"LLM interface must be provided for diagram generation.\" ) # Extract messages if present (Langchain ChatPromptValue) if hasattr ( prompt , \"to_messages\" ): messages = prompt . to_messages () if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif hasattr ( prompt , \"messages\" ): messages = prompt . messages if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif isinstance ( prompt , str ): prompt_input = prompt else : raise TypeError ( \"Prompt is not a recognized type for LLM input.\" ) response = llm_interface . invoke ( prompt_input ) diagram_code = self . check_content_type ( response ) return diagram_code except Exception as exc : retry_manager . record_error ( exc ) logging . exception ( \"UML diagram generation failed (attempt %s )\" , retry_manager . attempt ) # After max retries, raise with error context error_msg = ( f \"UML diagram generation failed after { retry_manager . max_retries } attempts. \\n \" f \"Error context: \\n { retry_manager . error_context () } \" ) raise RuntimeError ( error_msg )","title":"UMLDraftHandler"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLDraftHandler.__init__","text":"Initialize the handler with optional configuration overrides. Source code in UMLBot/uml_draft_handler.py 76 77 78 79 80 81 82 def __init__ ( self , config : Optional [ UMLBotConfig ] = None ): \"\"\"Initialize the handler with optional configuration overrides.\"\"\" super () . __init__ () self . prompty_path = ( Path ( __file__ ) . resolve () . parent . parent / \"assets\" / \"uml_diagram.prompty\" ) self . config = config or UMLBotConfig ()","title":"__init__"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLDraftHandler.construct_prompt","text":"Loads the prompty template and constructs the prompt with provided variables. Parameters: diagram_type ( str ) \u2013 Type of UML diagram to generate. description ( str ) \u2013 Description of the system/process to diagram. theme ( Optional [ str ] , default: None ) \u2013 PlantUML theme/style. Returns: ChatPromptTemplate ( Any ) \u2013 Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError \u2013 If prompty file is missing. ValueError \u2013 If prompty file is invalid. Source code in UMLBot/uml_draft_handler.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def construct_prompt ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , ) -> Any : \"\"\" Loads the prompty template and constructs the prompt with provided variables. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. Returns: ChatPromptTemplate: Assembled prompt template ready for LLM invocation. Raises: FileNotFoundError: If prompty file is missing. ValueError: If prompty file is invalid. \"\"\" prompt_template = self . load_prompty () # Ensure prompt_template is a ChatPromptTemplate and validate its template # Skipping prompt template validation for now: # if hasattr(self, \"_validate_prompt_template\"): # self._validate_prompt_template(prompt_template.template) variables = { \"diagram_type\" : escape_curly_braces ( diagram_type ), \"description\" : escape_curly_braces ( description ), \"theme\" : escape_curly_braces ( theme ), } return prompt_template . format_prompt ( ** variables )","title":"construct_prompt"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLDraftHandler.process","text":"Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Parameters: diagram_type ( str ) \u2013 Type of UML diagram to generate. description ( str ) \u2013 Description of the system/process to diagram. theme ( Optional [ str ] , default: None ) \u2013 PlantUML theme/style. llm_interface ( Optional [ Any ] , default: None ) \u2013 Optional LLM interface for invocation (must have .invoke()). retry_manager ( Optional [ UMLRetryManager ] , default: None ) \u2013 Helper for tracking retries and error context. Returns: str ( str ) \u2013 PlantUML diagram code. Raises: RuntimeError \u2013 If all retries fail, with error context. Source code in UMLBot/uml_draft_handler.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def process ( self , diagram_type : str , description : str , theme : Optional [ str ] = None , llm_interface : Optional [ Any ] = None , retry_manager : Optional [ \"UMLRetryManager\" ] = None , ) -> str : \"\"\" Generates a UML diagram using the LLM and prompty template, with error handling and retry logic. Args: diagram_type (str): Type of UML diagram to generate. description (str): Description of the system/process to diagram. theme (Optional[str]): PlantUML theme/style. llm_interface: Optional LLM interface for invocation (must have .invoke()). retry_manager (Optional[UMLRetryManager]): Helper for tracking retries and error context. Returns: str: PlantUML diagram code. Raises: RuntimeError: If all retries fail, with error context. \"\"\" if retry_manager is None : retry_manager = UMLRetryManager ( max_retries = 3 ) while retry_manager . should_retry (): try : prompt = self . construct_prompt ( diagram_type , description , theme ) if llm_interface is None : raise ValueError ( \"LLM interface must be provided for diagram generation.\" ) # Extract messages if present (Langchain ChatPromptValue) if hasattr ( prompt , \"to_messages\" ): messages = prompt . to_messages () if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif hasattr ( prompt , \"messages\" ): messages = prompt . messages if messages and hasattr ( messages [ 0 ], \"content\" ): prompt_input = messages [ 0 ] . content else : prompt_input = messages elif isinstance ( prompt , str ): prompt_input = prompt else : raise TypeError ( \"Prompt is not a recognized type for LLM input.\" ) response = llm_interface . invoke ( prompt_input ) diagram_code = self . check_content_type ( response ) return diagram_code except Exception as exc : retry_manager . record_error ( exc ) logging . exception ( \"UML diagram generation failed (attempt %s )\" , retry_manager . attempt ) # After max retries, raise with error context error_msg = ( f \"UML diagram generation failed after { retry_manager . max_retries } attempts. \\n \" f \"Error context: \\n { retry_manager . error_context () } \" ) raise RuntimeError ( error_msg )","title":"process"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager","text":"Tracks retry count and error context for UML rendering attempts. Attributes: max_retries ( int ) \u2013 Maximum number of retries allowed. attempt ( int ) \u2013 Current attempt number. errors ( list [ str ] ) \u2013 List of error messages from each failed attempt. Source code in UMLBot/uml_draft_handler.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class UMLRetryManager : \"\"\" Tracks retry count and error context for UML rendering attempts. Attributes: max_retries (int): Maximum number of retries allowed. attempt (int): Current attempt number. errors (list[str]): List of error messages from each failed attempt. \"\"\" def __init__ ( self , max_retries : int = 3 ) -> None : \"\"\"Initialize the retry manager with a maximum retry count.\"\"\" self . max_retries = max_retries self . attempt = 0 self . errors : list [ str ] = [] def record_error ( self , error : Exception ) -> None : \"\"\"Record an error and increment the attempt counter.\"\"\" self . attempt += 1 self . errors . append ( str ( error )) def should_retry ( self ) -> bool : \"\"\"Return True when another retry attempt is allowed.\"\"\" return self . attempt < self . max_retries def last_error ( self ) -> str : \"\"\"Return the most recent error message, or empty string if none.\"\"\" return self . errors [ - 1 ] if self . errors else \"\" def error_context ( self ) -> str : \"\"\"Return a multi-line summary of attempts and errors.\"\"\" return \" \\n \" . join ( f \"Attempt { i + 1 } : { msg } \" for i , msg in enumerate ( self . errors ))","title":"UMLRetryManager"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager.__init__","text":"Initialize the retry manager with a maximum retry count. Source code in UMLBot/uml_draft_handler.py 22 23 24 25 26 def __init__ ( self , max_retries : int = 3 ) -> None : \"\"\"Initialize the retry manager with a maximum retry count.\"\"\" self . max_retries = max_retries self . attempt = 0 self . errors : list [ str ] = []","title":"__init__"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager.error_context","text":"Return a multi-line summary of attempts and errors. Source code in UMLBot/uml_draft_handler.py 41 42 43 def error_context ( self ) -> str : \"\"\"Return a multi-line summary of attempts and errors.\"\"\" return \" \\n \" . join ( f \"Attempt { i + 1 } : { msg } \" for i , msg in enumerate ( self . errors ))","title":"error_context"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager.last_error","text":"Return the most recent error message, or empty string if none. Source code in UMLBot/uml_draft_handler.py 37 38 39 def last_error ( self ) -> str : \"\"\"Return the most recent error message, or empty string if none.\"\"\" return self . errors [ - 1 ] if self . errors else \"\"","title":"last_error"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager.record_error","text":"Record an error and increment the attempt counter. Source code in UMLBot/uml_draft_handler.py 28 29 30 31 def record_error ( self , error : Exception ) -> None : \"\"\"Record an error and increment the attempt counter.\"\"\" self . attempt += 1 self . errors . append ( str ( error ))","title":"record_error"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.UMLRetryManager.should_retry","text":"Return True when another retry attempt is allowed. Source code in UMLBot/uml_draft_handler.py 33 34 35 def should_retry ( self ) -> bool : \"\"\"Return True when another retry attempt is allowed.\"\"\" return self . attempt < self . max_retries","title":"should_retry"},{"location":"UMLBot/uml_draft_handler.html#UMLBot.uml_draft_handler.escape_curly_braces","text":"Escapes all curly braces in a string for safe prompt injection. Args: val (Optional[str]): Input string. Returns: Optional[str]: String with '{' replaced by '{{' and '}' replaced by '}}'. Source code in UMLBot/uml_draft_handler.py 51 52 53 54 55 56 57 58 59 60 61 def escape_curly_braces ( val : Optional [ str ]) -> Optional [ str ]: \"\"\" Escapes all curly braces in a string for safe prompt injection. Args: val (Optional[str]): Input string. Returns: Optional[str]: String with '{' replaced by '{{' and '}' replaced by '}}'. \"\"\" if val is None : return val return val . replace ( \"{\" , \"{{\" ) . replace ( \"}\" , \"}}\" )","title":"escape_curly_braces"},{"location":"UMLBot/config/__init__.html","text":"Configuration exports for UMLBot. UMLBotConfig Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError \u2013 If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also llm_utils.aiweb_common.WorkflowHandler.manage_sensitive Source code in UMLBot/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class UMLBotConfig : \"\"\" Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example: try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError: If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template: BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also: llm_utils.aiweb_common.WorkflowHandler.manage_sensitive \"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( os . getenv ( \"UMLBOT_DATA_DIR\" , \"/data\" )) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. HEADER_MARKDOWN = \"\"\"# EXAMPLE Header Markdown \\r todo - update this \"\"\" EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # UML Diagram Generator App Configs DIAGRAM_TYPES = [ \"Use Case\" , \"Class\" , \"Activity\" , \"Component\" , \"Deployment\" , \"State Machine\" , \"Timing\" , \"Sequence\" , ] DEFAULT_DIAGRAM_TYPE = \"Use Case\" DEFAULT_INPUT = \"Test\" SYSTEM_PROMPT_TEMPLATE = ( \"Convert the following description into a PlantUML {diagram_type} diagram. \\n {input} \" ) FALLBACK_PLANTUML_TEMPLATE = \"@startuml \\n ' {diagram_type} diagram \\n ' {description} \\n @enduml\" API_KEY_MISSING_MSG = \"OpenAI API key not found. Please ensure it is available via /run/secrets, /workspaces/*/secrets, or as an environment variable.\" DIAGRAM_SUCCESS_MSG = \"Diagram generated successfully using LLM.\" PLANTUML_SERVER_URL_TEMPLATE = os . getenv ( \"UMLBOT_PLANTUML_SERVER_URL_TEMPLATE\" , \"http://localhost:8080/png/ {encoded} \" , ) # LLM Configuration # For security, prefer to set LLM_API_KEY as an environment variable. try : LLM_API_KEY = manage_sensitive ( \"azure_proxy_key\" ) except KeyError : LLM_API_KEY = os . getenv ( \"UMLBOT_LLM_API_KEY\" , \"\" ) LLM_MODEL = os . getenv ( \"UMLBOT_LLM_MODEL\" , \"gpt-4o-mini\" ) LLM_API_BASE = os . getenv ( \"UMLBOT_LLM_API_BASE\" , \"\" ) CORS_ALLOW_ORIGINS = os . getenv ( \"UMLBOT_CORS_ALLOW_ORIGINS\" , \"\" ) if CORS_ALLOW_ORIGINS : CORS_ALLOW_ORIGINS = [ origin . strip () for origin in CORS_ALLOW_ORIGINS . split ( \",\" )] else : CORS_ALLOW_ORIGINS = [ \"http://localhost:3000\" , \"http://127.0.0.1:3000\" ]","title":"Init"},{"location":"UMLBot/config/__init__.html#UMLBot.config.UMLBotConfig","text":"Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError \u2013 If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also llm_utils.aiweb_common.WorkflowHandler.manage_sensitive Source code in UMLBot/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class UMLBotConfig : \"\"\" Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example: try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError: If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template: BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also: llm_utils.aiweb_common.WorkflowHandler.manage_sensitive \"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( os . getenv ( \"UMLBOT_DATA_DIR\" , \"/data\" )) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. HEADER_MARKDOWN = \"\"\"# EXAMPLE Header Markdown \\r todo - update this \"\"\" EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # UML Diagram Generator App Configs DIAGRAM_TYPES = [ \"Use Case\" , \"Class\" , \"Activity\" , \"Component\" , \"Deployment\" , \"State Machine\" , \"Timing\" , \"Sequence\" , ] DEFAULT_DIAGRAM_TYPE = \"Use Case\" DEFAULT_INPUT = \"Test\" SYSTEM_PROMPT_TEMPLATE = ( \"Convert the following description into a PlantUML {diagram_type} diagram. \\n {input} \" ) FALLBACK_PLANTUML_TEMPLATE = \"@startuml \\n ' {diagram_type} diagram \\n ' {description} \\n @enduml\" API_KEY_MISSING_MSG = \"OpenAI API key not found. Please ensure it is available via /run/secrets, /workspaces/*/secrets, or as an environment variable.\" DIAGRAM_SUCCESS_MSG = \"Diagram generated successfully using LLM.\" PLANTUML_SERVER_URL_TEMPLATE = os . getenv ( \"UMLBOT_PLANTUML_SERVER_URL_TEMPLATE\" , \"http://localhost:8080/png/ {encoded} \" , ) # LLM Configuration # For security, prefer to set LLM_API_KEY as an environment variable. try : LLM_API_KEY = manage_sensitive ( \"azure_proxy_key\" ) except KeyError : LLM_API_KEY = os . getenv ( \"UMLBOT_LLM_API_KEY\" , \"\" ) LLM_MODEL = os . getenv ( \"UMLBOT_LLM_MODEL\" , \"gpt-4o-mini\" ) LLM_API_BASE = os . getenv ( \"UMLBOT_LLM_API_BASE\" , \"\" ) CORS_ALLOW_ORIGINS = os . getenv ( \"UMLBOT_CORS_ALLOW_ORIGINS\" , \"\" ) if CORS_ALLOW_ORIGINS : CORS_ALLOW_ORIGINS = [ origin . strip () for origin in CORS_ALLOW_ORIGINS . split ( \",\" )] else : CORS_ALLOW_ORIGINS = [ \"http://localhost:3000\" , \"http://127.0.0.1:3000\" ]","title":"UMLBotConfig"},{"location":"UMLBot/config/config.html","text":"Application configuration for UMLBot. UMLBotConfig Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError \u2013 If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also llm_utils.aiweb_common.WorkflowHandler.manage_sensitive Source code in UMLBot/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class UMLBotConfig : \"\"\" Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example: try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError: If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template: BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also: llm_utils.aiweb_common.WorkflowHandler.manage_sensitive \"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( os . getenv ( \"UMLBOT_DATA_DIR\" , \"/data\" )) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. HEADER_MARKDOWN = \"\"\"# EXAMPLE Header Markdown \\r todo - update this \"\"\" EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # UML Diagram Generator App Configs DIAGRAM_TYPES = [ \"Use Case\" , \"Class\" , \"Activity\" , \"Component\" , \"Deployment\" , \"State Machine\" , \"Timing\" , \"Sequence\" , ] DEFAULT_DIAGRAM_TYPE = \"Use Case\" DEFAULT_INPUT = \"Test\" SYSTEM_PROMPT_TEMPLATE = ( \"Convert the following description into a PlantUML {diagram_type} diagram. \\n {input} \" ) FALLBACK_PLANTUML_TEMPLATE = \"@startuml \\n ' {diagram_type} diagram \\n ' {description} \\n @enduml\" API_KEY_MISSING_MSG = \"OpenAI API key not found. Please ensure it is available via /run/secrets, /workspaces/*/secrets, or as an environment variable.\" DIAGRAM_SUCCESS_MSG = \"Diagram generated successfully using LLM.\" PLANTUML_SERVER_URL_TEMPLATE = os . getenv ( \"UMLBOT_PLANTUML_SERVER_URL_TEMPLATE\" , \"http://localhost:8080/png/ {encoded} \" , ) # LLM Configuration # For security, prefer to set LLM_API_KEY as an environment variable. try : LLM_API_KEY = manage_sensitive ( \"azure_proxy_key\" ) except KeyError : LLM_API_KEY = os . getenv ( \"UMLBOT_LLM_API_KEY\" , \"\" ) LLM_MODEL = os . getenv ( \"UMLBOT_LLM_MODEL\" , \"gpt-4o-mini\" ) LLM_API_BASE = os . getenv ( \"UMLBOT_LLM_API_BASE\" , \"\" ) CORS_ALLOW_ORIGINS = os . getenv ( \"UMLBOT_CORS_ALLOW_ORIGINS\" , \"\" ) if CORS_ALLOW_ORIGINS : CORS_ALLOW_ORIGINS = [ origin . strip () for origin in CORS_ALLOW_ORIGINS . split ( \",\" )] else : CORS_ALLOW_ORIGINS = [ \"http://localhost:3000\" , \"http://127.0.0.1:3000\" ]","title":"Config"},{"location":"UMLBot/config/config.html#UMLBot.config.config.UMLBotConfig","text":"Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError \u2013 If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also llm_utils.aiweb_common.WorkflowHandler.manage_sensitive Source code in UMLBot/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class UMLBotConfig : \"\"\" Configuration class for UMLBot. Sensitive values (e.g., LLM_API_KEY) are retrieved using manage_sensitive for secure handling. If the secret is not found, a default value is used. Example: try: api_key = manage_sensitive(\"OPENAI_API_KEY\") except KeyError: api_key = \"sk-...\" Raises: KeyError: If manage_sensitive is called and the secret is not found, unless fallback is provided. Directories prepopulated in the template: BASE_DIR, CONFIG_DIR, LOGS_DIR, DATA_DIR, RAW_DATA, INTERMEDIATE_DIR, RESULTS_DIR See Also: llm_utils.aiweb_common.WorkflowHandler.manage_sensitive \"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( os . getenv ( \"UMLBOT_DATA_DIR\" , \"/data\" )) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. HEADER_MARKDOWN = \"\"\"# EXAMPLE Header Markdown \\r todo - update this \"\"\" EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # UML Diagram Generator App Configs DIAGRAM_TYPES = [ \"Use Case\" , \"Class\" , \"Activity\" , \"Component\" , \"Deployment\" , \"State Machine\" , \"Timing\" , \"Sequence\" , ] DEFAULT_DIAGRAM_TYPE = \"Use Case\" DEFAULT_INPUT = \"Test\" SYSTEM_PROMPT_TEMPLATE = ( \"Convert the following description into a PlantUML {diagram_type} diagram. \\n {input} \" ) FALLBACK_PLANTUML_TEMPLATE = \"@startuml \\n ' {diagram_type} diagram \\n ' {description} \\n @enduml\" API_KEY_MISSING_MSG = \"OpenAI API key not found. Please ensure it is available via /run/secrets, /workspaces/*/secrets, or as an environment variable.\" DIAGRAM_SUCCESS_MSG = \"Diagram generated successfully using LLM.\" PLANTUML_SERVER_URL_TEMPLATE = os . getenv ( \"UMLBOT_PLANTUML_SERVER_URL_TEMPLATE\" , \"http://localhost:8080/png/ {encoded} \" , ) # LLM Configuration # For security, prefer to set LLM_API_KEY as an environment variable. try : LLM_API_KEY = manage_sensitive ( \"azure_proxy_key\" ) except KeyError : LLM_API_KEY = os . getenv ( \"UMLBOT_LLM_API_KEY\" , \"\" ) LLM_MODEL = os . getenv ( \"UMLBOT_LLM_MODEL\" , \"gpt-4o-mini\" ) LLM_API_BASE = os . getenv ( \"UMLBOT_LLM_API_BASE\" , \"\" ) CORS_ALLOW_ORIGINS = os . getenv ( \"UMLBOT_CORS_ALLOW_ORIGINS\" , \"\" ) if CORS_ALLOW_ORIGINS : CORS_ALLOW_ORIGINS = [ origin . strip () for origin in CORS_ALLOW_ORIGINS . split ( \",\" )] else : CORS_ALLOW_ORIGINS = [ \"http://localhost:3000\" , \"http://127.0.0.1:3000\" ]","title":"UMLBotConfig"},{"location":"UMLBot/services/__init__.html","text":"Service layer exports for diagram generation. DiagramGenerationResult dataclass Result container for a UML generation request. Source code in UMLBot/services/diagram_service.py 22 23 24 25 26 27 28 @dataclass class DiagramGenerationResult : \"\"\"Result container for a UML generation request.\"\"\" plantuml_code : str pil_image : Image . Image | None status_message : str image_url : str diagram_image_to_base64 ( image ) Encode a PIL image to a base64 PNG string. Source code in UMLBot/services/diagram_service.py 108 109 110 111 112 113 114 115 def diagram_image_to_base64 ( image : Image . Image | None ) -> Optional [ str ]: \"\"\"Encode a PIL image to a base64 PNG string.\"\"\" if image is None : return None buf = io . BytesIO () image . save ( buf , format = \"PNG\" ) buf . seek ( 0 ) return base64 . b64encode ( buf . read ()) . decode ( \"utf-8\" ) generate_diagram_from_description ( description , diagram_type , theme = None ) Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. Source code in UMLBot/services/diagram_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def generate_diagram_from_description ( description : str , diagram_type : str , theme : Optional [ str ] = None , ) -> DiagramGenerationResult : \"\"\" Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. \"\"\" try : api_key = UMLBotConfig . LLM_API_KEY except KeyError : api_key = \"\" if not api_key or not UMLBotConfig . LLM_API_BASE : return DiagramGenerationResult ( plantuml_code = \"\" , pil_image = None , status_message = UMLBotConfig . API_KEY_MISSING_MSG , image_url = \"\" , ) handler = UMLDraftHandler () handler . _init_openai ( openai_compatible_endpoint = UMLBotConfig . LLM_API_BASE , openai_compatible_key = api_key , openai_compatible_model = UMLBotConfig . LLM_MODEL , name = \"UMLBot\" , ) try : plantuml_code = handler . process ( diagram_type = diagram_type , description = description , theme = theme , llm_interface = handler . llm_interface , ) status_msg = UMLBotConfig . DIAGRAM_SUCCESS_MSG except Exception as exc : LOGGER . exception ( \"LLM-backed generation failed, returning fallback diagram.\" ) plantuml_code = UMLBotConfig . FALLBACK_PLANTUML_TEMPLATE . format ( diagram_type = diagram_type , description = description , ) status_msg = f \"LLM error: { exc } . Showing fallback stub.\" cleaned_code = _strip_code_block_markers ( plantuml_code ) normalized_code = _normalize_curly_braces ( cleaned_code ) image_url = build_plantuml_image_url ( normalized_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = status_msg , ) if pil_image is None : image_url = \"\" return DiagramGenerationResult ( plantuml_code = normalized_code , pil_image = pil_image , status_message = status_msg , image_url = image_url , ) render_diagram_from_code ( plantuml_code ) Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. Source code in UMLBot/services/diagram_service.py 93 94 95 96 97 98 99 100 101 102 103 104 105 def render_diagram_from_code ( plantuml_code : str ) -> Tuple [ Image . Image , str , str ]: \"\"\" Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. \"\"\" image_url = build_plantuml_image_url ( plantuml_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = \"Re-rendered from PlantUML code.\" , ) if pil_image is None : pil_image = _create_placeholder_image ( \"Diagram preview unavailable\" ) return pil_image , status_msg , image_url","title":"Init"},{"location":"UMLBot/services/__init__.html#UMLBot.services.DiagramGenerationResult","text":"Result container for a UML generation request. Source code in UMLBot/services/diagram_service.py 22 23 24 25 26 27 28 @dataclass class DiagramGenerationResult : \"\"\"Result container for a UML generation request.\"\"\" plantuml_code : str pil_image : Image . Image | None status_message : str image_url : str","title":"DiagramGenerationResult"},{"location":"UMLBot/services/__init__.html#UMLBot.services.diagram_image_to_base64","text":"Encode a PIL image to a base64 PNG string. Source code in UMLBot/services/diagram_service.py 108 109 110 111 112 113 114 115 def diagram_image_to_base64 ( image : Image . Image | None ) -> Optional [ str ]: \"\"\"Encode a PIL image to a base64 PNG string.\"\"\" if image is None : return None buf = io . BytesIO () image . save ( buf , format = \"PNG\" ) buf . seek ( 0 ) return base64 . b64encode ( buf . read ()) . decode ( \"utf-8\" )","title":"diagram_image_to_base64"},{"location":"UMLBot/services/__init__.html#UMLBot.services.generate_diagram_from_description","text":"Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. Source code in UMLBot/services/diagram_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def generate_diagram_from_description ( description : str , diagram_type : str , theme : Optional [ str ] = None , ) -> DiagramGenerationResult : \"\"\" Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. \"\"\" try : api_key = UMLBotConfig . LLM_API_KEY except KeyError : api_key = \"\" if not api_key or not UMLBotConfig . LLM_API_BASE : return DiagramGenerationResult ( plantuml_code = \"\" , pil_image = None , status_message = UMLBotConfig . API_KEY_MISSING_MSG , image_url = \"\" , ) handler = UMLDraftHandler () handler . _init_openai ( openai_compatible_endpoint = UMLBotConfig . LLM_API_BASE , openai_compatible_key = api_key , openai_compatible_model = UMLBotConfig . LLM_MODEL , name = \"UMLBot\" , ) try : plantuml_code = handler . process ( diagram_type = diagram_type , description = description , theme = theme , llm_interface = handler . llm_interface , ) status_msg = UMLBotConfig . DIAGRAM_SUCCESS_MSG except Exception as exc : LOGGER . exception ( \"LLM-backed generation failed, returning fallback diagram.\" ) plantuml_code = UMLBotConfig . FALLBACK_PLANTUML_TEMPLATE . format ( diagram_type = diagram_type , description = description , ) status_msg = f \"LLM error: { exc } . Showing fallback stub.\" cleaned_code = _strip_code_block_markers ( plantuml_code ) normalized_code = _normalize_curly_braces ( cleaned_code ) image_url = build_plantuml_image_url ( normalized_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = status_msg , ) if pil_image is None : image_url = \"\" return DiagramGenerationResult ( plantuml_code = normalized_code , pil_image = pil_image , status_message = status_msg , image_url = image_url , )","title":"generate_diagram_from_description"},{"location":"UMLBot/services/__init__.html#UMLBot.services.render_diagram_from_code","text":"Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. Source code in UMLBot/services/diagram_service.py 93 94 95 96 97 98 99 100 101 102 103 104 105 def render_diagram_from_code ( plantuml_code : str ) -> Tuple [ Image . Image , str , str ]: \"\"\" Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. \"\"\" image_url = build_plantuml_image_url ( plantuml_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = \"Re-rendered from PlantUML code.\" , ) if pil_image is None : pil_image = _create_placeholder_image ( \"Diagram preview unavailable\" ) return pil_image , status_msg , image_url","title":"render_diagram_from_code"},{"location":"UMLBot/services/diagram_service.html","text":"Diagram generation services and PlantUML rendering helpers. DiagramGenerationResult dataclass Result container for a UML generation request. Source code in UMLBot/services/diagram_service.py 22 23 24 25 26 27 28 @dataclass class DiagramGenerationResult : \"\"\"Result container for a UML generation request.\"\"\" plantuml_code : str pil_image : Image . Image | None status_message : str image_url : str build_plantuml_image_url ( plantuml_code ) Build a PlantUML render URL for the given diagram code. Source code in UMLBot/services/diagram_service.py 118 119 120 121 def build_plantuml_image_url ( plantuml_code : str ) -> str : \"\"\"Build a PlantUML render URL for the given diagram code.\"\"\" encoded = _plantuml_encode ( plantuml_code ) return UMLBotConfig . PLANTUML_SERVER_URL_TEMPLATE . format ( encoded = encoded ) diagram_image_to_base64 ( image ) Encode a PIL image to a base64 PNG string. Source code in UMLBot/services/diagram_service.py 108 109 110 111 112 113 114 115 def diagram_image_to_base64 ( image : Image . Image | None ) -> Optional [ str ]: \"\"\"Encode a PIL image to a base64 PNG string.\"\"\" if image is None : return None buf = io . BytesIO () image . save ( buf , format = \"PNG\" ) buf . seek ( 0 ) return base64 . b64encode ( buf . read ()) . decode ( \"utf-8\" ) generate_diagram_from_description ( description , diagram_type , theme = None ) Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. Source code in UMLBot/services/diagram_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def generate_diagram_from_description ( description : str , diagram_type : str , theme : Optional [ str ] = None , ) -> DiagramGenerationResult : \"\"\" Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. \"\"\" try : api_key = UMLBotConfig . LLM_API_KEY except KeyError : api_key = \"\" if not api_key or not UMLBotConfig . LLM_API_BASE : return DiagramGenerationResult ( plantuml_code = \"\" , pil_image = None , status_message = UMLBotConfig . API_KEY_MISSING_MSG , image_url = \"\" , ) handler = UMLDraftHandler () handler . _init_openai ( openai_compatible_endpoint = UMLBotConfig . LLM_API_BASE , openai_compatible_key = api_key , openai_compatible_model = UMLBotConfig . LLM_MODEL , name = \"UMLBot\" , ) try : plantuml_code = handler . process ( diagram_type = diagram_type , description = description , theme = theme , llm_interface = handler . llm_interface , ) status_msg = UMLBotConfig . DIAGRAM_SUCCESS_MSG except Exception as exc : LOGGER . exception ( \"LLM-backed generation failed, returning fallback diagram.\" ) plantuml_code = UMLBotConfig . FALLBACK_PLANTUML_TEMPLATE . format ( diagram_type = diagram_type , description = description , ) status_msg = f \"LLM error: { exc } . Showing fallback stub.\" cleaned_code = _strip_code_block_markers ( plantuml_code ) normalized_code = _normalize_curly_braces ( cleaned_code ) image_url = build_plantuml_image_url ( normalized_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = status_msg , ) if pil_image is None : image_url = \"\" return DiagramGenerationResult ( plantuml_code = normalized_code , pil_image = pil_image , status_message = status_msg , image_url = image_url , ) render_diagram_from_code ( plantuml_code ) Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. Source code in UMLBot/services/diagram_service.py 93 94 95 96 97 98 99 100 101 102 103 104 105 def render_diagram_from_code ( plantuml_code : str ) -> Tuple [ Image . Image , str , str ]: \"\"\" Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. \"\"\" image_url = build_plantuml_image_url ( plantuml_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = \"Re-rendered from PlantUML code.\" , ) if pil_image is None : pil_image = _create_placeholder_image ( \"Diagram preview unavailable\" ) return pil_image , status_msg , image_url","title":"Diagram Service"},{"location":"UMLBot/services/diagram_service.html#UMLBot.services.diagram_service.DiagramGenerationResult","text":"Result container for a UML generation request. Source code in UMLBot/services/diagram_service.py 22 23 24 25 26 27 28 @dataclass class DiagramGenerationResult : \"\"\"Result container for a UML generation request.\"\"\" plantuml_code : str pil_image : Image . Image | None status_message : str image_url : str","title":"DiagramGenerationResult"},{"location":"UMLBot/services/diagram_service.html#UMLBot.services.diagram_service.build_plantuml_image_url","text":"Build a PlantUML render URL for the given diagram code. Source code in UMLBot/services/diagram_service.py 118 119 120 121 def build_plantuml_image_url ( plantuml_code : str ) -> str : \"\"\"Build a PlantUML render URL for the given diagram code.\"\"\" encoded = _plantuml_encode ( plantuml_code ) return UMLBotConfig . PLANTUML_SERVER_URL_TEMPLATE . format ( encoded = encoded )","title":"build_plantuml_image_url"},{"location":"UMLBot/services/diagram_service.html#UMLBot.services.diagram_service.diagram_image_to_base64","text":"Encode a PIL image to a base64 PNG string. Source code in UMLBot/services/diagram_service.py 108 109 110 111 112 113 114 115 def diagram_image_to_base64 ( image : Image . Image | None ) -> Optional [ str ]: \"\"\"Encode a PIL image to a base64 PNG string.\"\"\" if image is None : return None buf = io . BytesIO () image . save ( buf , format = \"PNG\" ) buf . seek ( 0 ) return base64 . b64encode ( buf . read ()) . decode ( \"utf-8\" )","title":"diagram_image_to_base64"},{"location":"UMLBot/services/diagram_service.html#UMLBot.services.diagram_service.generate_diagram_from_description","text":"Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. Source code in UMLBot/services/diagram_service.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def generate_diagram_from_description ( description : str , diagram_type : str , theme : Optional [ str ] = None , ) -> DiagramGenerationResult : \"\"\" Runs the UMLDraftHandler pipeline and returns the PlantUML code plus the rendered image. Errors are converted to a fallback PlantUML stub with contextual messaging. \"\"\" try : api_key = UMLBotConfig . LLM_API_KEY except KeyError : api_key = \"\" if not api_key or not UMLBotConfig . LLM_API_BASE : return DiagramGenerationResult ( plantuml_code = \"\" , pil_image = None , status_message = UMLBotConfig . API_KEY_MISSING_MSG , image_url = \"\" , ) handler = UMLDraftHandler () handler . _init_openai ( openai_compatible_endpoint = UMLBotConfig . LLM_API_BASE , openai_compatible_key = api_key , openai_compatible_model = UMLBotConfig . LLM_MODEL , name = \"UMLBot\" , ) try : plantuml_code = handler . process ( diagram_type = diagram_type , description = description , theme = theme , llm_interface = handler . llm_interface , ) status_msg = UMLBotConfig . DIAGRAM_SUCCESS_MSG except Exception as exc : LOGGER . exception ( \"LLM-backed generation failed, returning fallback diagram.\" ) plantuml_code = UMLBotConfig . FALLBACK_PLANTUML_TEMPLATE . format ( diagram_type = diagram_type , description = description , ) status_msg = f \"LLM error: { exc } . Showing fallback stub.\" cleaned_code = _strip_code_block_markers ( plantuml_code ) normalized_code = _normalize_curly_braces ( cleaned_code ) image_url = build_plantuml_image_url ( normalized_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = status_msg , ) if pil_image is None : image_url = \"\" return DiagramGenerationResult ( plantuml_code = normalized_code , pil_image = pil_image , status_message = status_msg , image_url = image_url , )","title":"generate_diagram_from_description"},{"location":"UMLBot/services/diagram_service.html#UMLBot.services.diagram_service.render_diagram_from_code","text":"Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. Source code in UMLBot/services/diagram_service.py 93 94 95 96 97 98 99 100 101 102 103 104 105 def render_diagram_from_code ( plantuml_code : str ) -> Tuple [ Image . Image , str , str ]: \"\"\" Re-renders an already generated PlantUML snippet. Returns a placeholder image if the render fails so the UI can continue gracefully. \"\"\" image_url = build_plantuml_image_url ( plantuml_code ) pil_image , status_msg = _fetch_plantuml_image ( image_url = image_url , status_msg = \"Re-rendered from PlantUML code.\" , ) if pil_image is None : pil_image = _create_placeholder_image ( \"Diagram preview unavailable\" ) return pil_image , status_msg , image_url","title":"render_diagram_from_code"},{"location":"UMLBot/utils/plantuml_extractor.html","text":"PlantUML Extraction Utility Extracts the last valid PlantUML code block from a chat response string. - Finds blocks delimited by triple backticks ( plantuml ... ) or containing @startuml. - Strips code block markers and whitespace. - Validates presence of @startuml and @enduml. Raises: ValueError \u2013 If no valid PlantUML block is found. Example extract_last_plantuml_block(\"... plantuml\\n@startuml ... @enduml\\n ...\") '@startuml ... @enduml' extract_last_plantuml_block ( text ) Extracts the last valid PlantUML code block from the input text. Parameters: text ( str ) \u2013 The chat response string. Returns: str ( str ) \u2013 The extracted PlantUML code. Raises: ValueError \u2013 If no valid PlantUML block is found. Source code in UMLBot/utils/plantuml_extractor.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def extract_last_plantuml_block ( text : str ) -> str : \"\"\" Extracts the last valid PlantUML code block from the input text. Args: text (str): The chat response string. Returns: str: The extracted PlantUML code. Raises: ValueError: If no valid PlantUML block is found. \"\"\" # Find all code blocks: ```plantuml ... ``` or ``` ... ``` code_block_pattern = re . compile ( r \"```(?:plantuml)?\\s*([\\s\\S]*?)```\" , re . MULTILINE ) blocks = code_block_pattern . findall ( text ) # Also find any @startuml ... @enduml blocks outside code blocks uml_pattern = re . compile ( r \"@startuml[\\s\\S]*?@enduml\" , re . MULTILINE ) blocks += uml_pattern . findall ( text ) # Filter for valid PlantUML blocks valid_blocks = [] for block in blocks : block_stripped = block . strip () # Remove code block markers if present block_stripped = re . sub ( r \"^```(?:plantuml)?\\s*|```$\" , \"\" , block_stripped , flags = re . MULTILINE ) . strip () if \"@startuml\" in block_stripped and \"@enduml\" in block_stripped : valid_blocks . append ( block_stripped ) if not valid_blocks : raise ValueError ( \"No valid PlantUML block found in response.\" ) # Return the last valid block return valid_blocks [ - 1 ]","title":"PlantUML Extractor"},{"location":"UMLBot/utils/plantuml_extractor.html#UMLBot.utils.plantuml_extractor.extract_last_plantuml_block","text":"Extracts the last valid PlantUML code block from the input text. Parameters: text ( str ) \u2013 The chat response string. Returns: str ( str ) \u2013 The extracted PlantUML code. Raises: ValueError \u2013 If no valid PlantUML block is found. Source code in UMLBot/utils/plantuml_extractor.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def extract_last_plantuml_block ( text : str ) -> str : \"\"\" Extracts the last valid PlantUML code block from the input text. Args: text (str): The chat response string. Returns: str: The extracted PlantUML code. Raises: ValueError: If no valid PlantUML block is found. \"\"\" # Find all code blocks: ```plantuml ... ``` or ``` ... ``` code_block_pattern = re . compile ( r \"```(?:plantuml)?\\s*([\\s\\S]*?)```\" , re . MULTILINE ) blocks = code_block_pattern . findall ( text ) # Also find any @startuml ... @enduml blocks outside code blocks uml_pattern = re . compile ( r \"@startuml[\\s\\S]*?@enduml\" , re . MULTILINE ) blocks += uml_pattern . findall ( text ) # Filter for valid PlantUML blocks valid_blocks = [] for block in blocks : block_stripped = block . strip () # Remove code block markers if present block_stripped = re . sub ( r \"^```(?:plantuml)?\\s*|```$\" , \"\" , block_stripped , flags = re . MULTILINE ) . strip () if \"@startuml\" in block_stripped and \"@enduml\" in block_stripped : valid_blocks . append ( block_stripped ) if not valid_blocks : raise ValueError ( \"No valid PlantUML block found in response.\" ) # Return the last valid block return valid_blocks [ - 1 ]","title":"extract_last_plantuml_block"},{"location":"llm_utils/index.html","text":"Common code documentation AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Home"},{"location":"llm_utils/index.html#common-code-documentation","text":"AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Common code documentation"},{"location":"llm_utils/aiweb_common/ObjectFactory.html","text":"ObjectFactory Source code in llm_utils/aiweb_common/ObjectFactory.py 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class ObjectFactory : def __init__ ( self ): self . _builders = {} def register_builder ( self , key , builder ): \"\"\" The `register_builder` function adds a builder object to a dictionary with a specified key. :param key: The `key` parameter in the `register_builder` method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The `register_builder` method is used to register a builder function with a specific key in the `_builders` dictionary. The `key` parameter is the identifier for the builder function, and the `builder` parameter is the actual function that will be stored in the dictionary under that key \"\"\" self . _builders [ key ] = builder def create ( self , key , ** kwargs ): \"\"\" The `create` function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The `key` parameter in the `create` method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the `_builders` dictionary based on the provided key :return: The `create` method returns the result of calling the builder function associated with the given key, passing in the keyword arguments `kwargs`. \"\"\" builder = self . _builders . get ( key ) if not builder : raise ValueError ( key ) return builder ( ** kwargs ) create ( key , ** kwargs ) The create function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The key parameter in the create method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the _builders dictionary based on the provided key :return: The create method returns the result of calling the builder function associated with the given key, passing in the keyword arguments kwargs . Source code in llm_utils/aiweb_common/ObjectFactory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def create ( self , key , ** kwargs ): \"\"\" The `create` function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The `key` parameter in the `create` method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the `_builders` dictionary based on the provided key :return: The `create` method returns the result of calling the builder function associated with the given key, passing in the keyword arguments `kwargs`. \"\"\" builder = self . _builders . get ( key ) if not builder : raise ValueError ( key ) return builder ( ** kwargs ) register_builder ( key , builder ) The register_builder function adds a builder object to a dictionary with a specified key. :param key: The key parameter in the register_builder method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The register_builder method is used to register a builder function with a specific key in the _builders dictionary. The key parameter is the identifier for the builder function, and the builder parameter is the actual function that will be stored in the dictionary under that key Source code in llm_utils/aiweb_common/ObjectFactory.py 6 7 8 9 10 11 12 13 14 15 16 17 18 def register_builder ( self , key , builder ): \"\"\" The `register_builder` function adds a builder object to a dictionary with a specified key. :param key: The `key` parameter in the `register_builder` method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The `register_builder` method is used to register a builder function with a specific key in the `_builders` dictionary. The `key` parameter is the identifier for the builder function, and the `builder` parameter is the actual function that will be stored in the dictionary under that key \"\"\" self . _builders [ key ] = builder","title":"Object Factory"},{"location":"llm_utils/aiweb_common/ObjectFactory.html#aiweb_common.ObjectFactory.ObjectFactory","text":"Source code in llm_utils/aiweb_common/ObjectFactory.py 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class ObjectFactory : def __init__ ( self ): self . _builders = {} def register_builder ( self , key , builder ): \"\"\" The `register_builder` function adds a builder object to a dictionary with a specified key. :param key: The `key` parameter in the `register_builder` method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The `register_builder` method is used to register a builder function with a specific key in the `_builders` dictionary. The `key` parameter is the identifier for the builder function, and the `builder` parameter is the actual function that will be stored in the dictionary under that key \"\"\" self . _builders [ key ] = builder def create ( self , key , ** kwargs ): \"\"\" The `create` function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The `key` parameter in the `create` method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the `_builders` dictionary based on the provided key :return: The `create` method returns the result of calling the builder function associated with the given key, passing in the keyword arguments `kwargs`. \"\"\" builder = self . _builders . get ( key ) if not builder : raise ValueError ( key ) return builder ( ** kwargs )","title":"ObjectFactory"},{"location":"llm_utils/aiweb_common/ObjectFactory.html#aiweb_common.ObjectFactory.ObjectFactory.create","text":"The create function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The key parameter in the create method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the _builders dictionary based on the provided key :return: The create method returns the result of calling the builder function associated with the given key, passing in the keyword arguments kwargs . Source code in llm_utils/aiweb_common/ObjectFactory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def create ( self , key , ** kwargs ): \"\"\" The `create` function takes a key and keyword arguments, retrieves a builder based on the key, and returns the result of calling the builder with the provided arguments. :param key: The `key` parameter in the `create` method is used to determine which builder function to use for creating an object. It is used to look up the appropriate builder function from the `_builders` dictionary based on the provided key :return: The `create` method returns the result of calling the builder function associated with the given key, passing in the keyword arguments `kwargs`. \"\"\" builder = self . _builders . get ( key ) if not builder : raise ValueError ( key ) return builder ( ** kwargs )","title":"create"},{"location":"llm_utils/aiweb_common/ObjectFactory.html#aiweb_common.ObjectFactory.ObjectFactory.register_builder","text":"The register_builder function adds a builder object to a dictionary with a specified key. :param key: The key parameter in the register_builder method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The register_builder method is used to register a builder function with a specific key in the _builders dictionary. The key parameter is the identifier for the builder function, and the builder parameter is the actual function that will be stored in the dictionary under that key Source code in llm_utils/aiweb_common/ObjectFactory.py 6 7 8 9 10 11 12 13 14 15 16 17 18 def register_builder ( self , key , builder ): \"\"\" The `register_builder` function adds a builder object to a dictionary with a specified key. :param key: The `key` parameter in the `register_builder` method is used as a unique identifier for the builder that is being registered. It is typically a string or any other hashable object that can be used as a key in a dictionary to store and retrieve the corresponding builder object :param builder: The `register_builder` method is used to register a builder function with a specific key in the `_builders` dictionary. The `key` parameter is the identifier for the builder function, and the `builder` parameter is the actual function that will be stored in the dictionary under that key \"\"\" self . _builders [ key ] = builder","title":"register_builder"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html","text":"WorkflowHandler Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _init_openai ( self , * , openai_compatible_endpoint , openai_compatible_key , openai_compatible_model , name ): self . llm_interface = ChatOpenAI ( base_url = openai_compatible_endpoint , api_key = openai_compatible_key , model = openai_compatible_model , user = name ) def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" # for compatibility, temporarily only import pyodc when needed. import pyodbc conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" # for compatibility, temporarily only import pyodc when needed. import pyodbc with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): print ( f \"DEBUG: check_content_type received type= { type ( returned_content ) } , value= { repr ( returned_content ) } \" ) # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content elif isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) log_to_database ( app_config , content_to_log , start , finish , background_tasks , label = '' ) This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) manage_sensitive ( name ) The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"Workflow Handler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler","text":"Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _init_openai ( self , * , openai_compatible_endpoint , openai_compatible_key , openai_compatible_model , name ): self . llm_interface = ChatOpenAI ( base_url = openai_compatible_endpoint , api_key = openai_compatible_key , model = openai_compatible_model , user = name ) def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" # for compatibility, temporarily only import pyodc when needed. import pyodbc conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" # for compatibility, temporarily only import pyodc when needed. import pyodbc with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): print ( f \"DEBUG: check_content_type received type= { type ( returned_content ) } , value= { repr ( returned_content ) } \" ) # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content elif isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"WorkflowHandler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler.log_to_database","text":"This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"log_to_database"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.manage_sensitive","text":"The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"manage_sensitive"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html","text":"convert_file_to_base64 ( file = File ( ... )) async Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) decode_to_file ( request , background_tasks ) async Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"Helper APIS"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.convert_file_to_base64","text":"Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"convert_file_to_base64"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.decode_to_file","text":"Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"decode_to_file"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html","text":"BibTexResponse Bases: BaseModel This class represents a response in BibTeX format. Source code in llm_utils/aiweb_common/fastapi/schemas.py 34 35 36 37 38 39 class BibTexResponse ( BaseModel ): \"\"\"This class represents a response in BibTeX format.\"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded BIB file. Decode to obtain the BIB file.\" ) JSONResponse Bases: BaseModel This class represents a response containing JSON data in a base64-encoded string. Decode this string to obtain the original JSON content. Source code in llm_utils/aiweb_common/fastapi/schemas.py 50 51 52 53 54 55 56 57 58 class JSONResponse ( BaseModel ): \"\"\" This class represents a response containing JSON data in a base64-encoded string. Decode this string to obtain the original JSON content. \"\"\" encoded_json : str = Field ( ... , description = \"Base64-encoded JSON data. Decode to obtain the JSON file.\" ) MSExcelResponse Bases: BaseModel This class likely represents a response from an API that interacts with Microsoft Excel files. Source code in llm_utils/aiweb_common/fastapi/schemas.py 42 43 44 45 46 47 class MSExcelResponse ( BaseModel ): \"\"\"This class likely represents a response from an API that interacts with Microsoft Excel files.\"\"\" encoded_xlsx : str = Field ( ... , description = \"Base64-encoded XLSX file. Decode to obtain the XLSX file.\" ) MSWordResponse Bases: BaseModel This class likely represents a response from a Microsoft Word document. Source code in llm_utils/aiweb_common/fastapi/schemas.py 26 27 28 29 30 31 class MSWordResponse ( BaseModel ): \"\"\"This class likely represents a response from a Microsoft Word document.\"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" ) SearchRequest Bases: BaseModel This class represents a search request in Python. Source code in llm_utils/aiweb_common/fastapi/schemas.py 20 21 22 23 class SearchRequest ( BaseModel ): \"\"\"This class represents a search request in Python.\"\"\" research_question : str UploadableFiles Bases: str , Enum The class UploadableFiles is a subclass of str and Enum in Python. Source code in llm_utils/aiweb_common/fastapi/schemas.py 8 9 10 11 12 class UploadableFiles ( str , Enum ): \"\"\"The class `UploadableFiles` is a subclass of `str` and `Enum` in Python.\"\"\" DOCX = \".docx\" XLSX = \".xlsx\" XLSXinRequest Bases: BaseModel This class likely represents a request object for handling XLSX file data in a Python application. Source code in llm_utils/aiweb_common/fastapi/schemas.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class XLSXinRequest ( BaseModel ): \"\"\"This class likely represents a request object for handling XLSX file data in a Python application.\"\"\" xlsx_encoded : str = Field ( ... , description = \"Base64-encoded XLSX file content.\" ) @field_validator ( \"xlsx_encoded\" ) @classmethod def check_mime_type ( cls , v , values , ** kwargs ): \"\"\" The function `check_mime_type` validates XLSX file bytes. Args: cls: In the provided code snippet, the `cls` parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use `cls` as the first parameter in class methods to represent the class object. v: The `v` parameter in the `check_mime_type` function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The `values` parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the `check_mime_type` function. Returns: The `check_mime_type` function is returning the result of calling the `validate_xlsx_bytes` function with the arguments `cls`, `v`, and `values`. \"\"\" return validate_xlsx_bytes ( cls , v , values ) check_mime_type ( v , values , ** kwargs ) classmethod The function check_mime_type validates XLSX file bytes. Parameters: cls \u2013 In the provided code snippet, the cls parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use cls as the first parameter in class methods to represent the class object. v: The v parameter in the check_mime_type function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The values parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the check_mime_type function. Returns: \u2013 The check_mime_type function is returning the result of calling the validate_xlsx_bytes function with the arguments cls , v , and values . Source code in llm_utils/aiweb_common/fastapi/schemas.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @field_validator ( \"xlsx_encoded\" ) @classmethod def check_mime_type ( cls , v , values , ** kwargs ): \"\"\" The function `check_mime_type` validates XLSX file bytes. Args: cls: In the provided code snippet, the `cls` parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use `cls` as the first parameter in class methods to represent the class object. v: The `v` parameter in the `check_mime_type` function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The `values` parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the `check_mime_type` function. Returns: The `check_mime_type` function is returning the result of calling the `validate_xlsx_bytes` function with the arguments `cls`, `v`, and `values`. \"\"\" return validate_xlsx_bytes ( cls , v , values )","title":"Schemas"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.BibTexResponse","text":"Bases: BaseModel This class represents a response in BibTeX format. Source code in llm_utils/aiweb_common/fastapi/schemas.py 34 35 36 37 38 39 class BibTexResponse ( BaseModel ): \"\"\"This class represents a response in BibTeX format.\"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded BIB file. Decode to obtain the BIB file.\" )","title":"BibTexResponse"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.JSONResponse","text":"Bases: BaseModel This class represents a response containing JSON data in a base64-encoded string. Decode this string to obtain the original JSON content. Source code in llm_utils/aiweb_common/fastapi/schemas.py 50 51 52 53 54 55 56 57 58 class JSONResponse ( BaseModel ): \"\"\" This class represents a response containing JSON data in a base64-encoded string. Decode this string to obtain the original JSON content. \"\"\" encoded_json : str = Field ( ... , description = \"Base64-encoded JSON data. Decode to obtain the JSON file.\" )","title":"JSONResponse"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.MSExcelResponse","text":"Bases: BaseModel This class likely represents a response from an API that interacts with Microsoft Excel files. Source code in llm_utils/aiweb_common/fastapi/schemas.py 42 43 44 45 46 47 class MSExcelResponse ( BaseModel ): \"\"\"This class likely represents a response from an API that interacts with Microsoft Excel files.\"\"\" encoded_xlsx : str = Field ( ... , description = \"Base64-encoded XLSX file. Decode to obtain the XLSX file.\" )","title":"MSExcelResponse"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.MSWordResponse","text":"Bases: BaseModel This class likely represents a response from a Microsoft Word document. Source code in llm_utils/aiweb_common/fastapi/schemas.py 26 27 28 29 30 31 class MSWordResponse ( BaseModel ): \"\"\"This class likely represents a response from a Microsoft Word document.\"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" )","title":"MSWordResponse"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.SearchRequest","text":"Bases: BaseModel This class represents a search request in Python. Source code in llm_utils/aiweb_common/fastapi/schemas.py 20 21 22 23 class SearchRequest ( BaseModel ): \"\"\"This class represents a search request in Python.\"\"\" research_question : str","title":"SearchRequest"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.UploadableFiles","text":"Bases: str , Enum The class UploadableFiles is a subclass of str and Enum in Python. Source code in llm_utils/aiweb_common/fastapi/schemas.py 8 9 10 11 12 class UploadableFiles ( str , Enum ): \"\"\"The class `UploadableFiles` is a subclass of `str` and `Enum` in Python.\"\"\" DOCX = \".docx\" XLSX = \".xlsx\"","title":"UploadableFiles"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.XLSXinRequest","text":"Bases: BaseModel This class likely represents a request object for handling XLSX file data in a Python application. Source code in llm_utils/aiweb_common/fastapi/schemas.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class XLSXinRequest ( BaseModel ): \"\"\"This class likely represents a request object for handling XLSX file data in a Python application.\"\"\" xlsx_encoded : str = Field ( ... , description = \"Base64-encoded XLSX file content.\" ) @field_validator ( \"xlsx_encoded\" ) @classmethod def check_mime_type ( cls , v , values , ** kwargs ): \"\"\" The function `check_mime_type` validates XLSX file bytes. Args: cls: In the provided code snippet, the `cls` parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use `cls` as the first parameter in class methods to represent the class object. v: The `v` parameter in the `check_mime_type` function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The `values` parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the `check_mime_type` function. Returns: The `check_mime_type` function is returning the result of calling the `validate_xlsx_bytes` function with the arguments `cls`, `v`, and `values`. \"\"\" return validate_xlsx_bytes ( cls , v , values )","title":"XLSXinRequest"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.XLSXinRequest.check_mime_type","text":"The function check_mime_type validates XLSX file bytes. Parameters: cls \u2013 In the provided code snippet, the cls parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use cls as the first parameter in class methods to represent the class object. v: The v parameter in the check_mime_type function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The values parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the check_mime_type function. Returns: \u2013 The check_mime_type function is returning the result of calling the validate_xlsx_bytes function with the arguments cls , v , and values . Source code in llm_utils/aiweb_common/fastapi/schemas.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @field_validator ( \"xlsx_encoded\" ) @classmethod def check_mime_type ( cls , v , values , ** kwargs ): \"\"\" The function `check_mime_type` validates XLSX file bytes. Args: cls: In the provided code snippet, the `cls` parameter is typically used to refer to the class itself within a class method. It is a common convention in Python to use `cls` as the first parameter in class methods to represent the class object. v: The `v` parameter in the `check_mime_type` function likely represents the value that needs to be validated or checked for its MIME type. It is passed as an argument to the function when it is called. values: The `values` parameter typically refers to a dictionary containing all the values being passed to the function or method. In this context, it is likely used to pass additional information or context to the `check_mime_type` function. Returns: The `check_mime_type` function is returning the result of calling the `validate_xlsx_bytes` function with the arguments `cls`, `v`, and `values`. \"\"\" return validate_xlsx_bytes ( cls , v , values )","title":"check_mime_type"},{"location":"llm_utils/aiweb_common/fastapi/validators.html","text":"","title":"Validators"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html","text":"DocxCreator Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , summary = None , results = None , figures = None ): self . results = results self . figures = figures self . summary = summary def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" , ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_figures ( self , doc : Document ): # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) buf . seek ( 0 ) doc . add_picture ( buf , width = Inches ( 5 )) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : Mapping [ str , Mapping [ str , object ]], * , order : tuple [ str , ... ] | None = None , pretty_names : Mapping [ str , str ] | None = None , ) -> None : \"\"\" Insert *all* results into the DOCX, no hard-coded section names required. Parameters ---------- doc : python-docx Document The document to write into. results : dict[str, dict[str, Any]] Outer keys = method names. Inner keys = arbitrary section names whose values can be rendered by ``self._add_table``. order : tuple[str] | None, default None If given, section keys in this tuple are shown first and in exactly this order; any remaining keys follow in their natural order. pretty_names : dict[str, str] | None, default None Optional mapping that converts raw section keys into nicer titles. Anything not listed falls back to `title_case(key)`. \"\"\" pretty_names = pretty_names or {} order = order or () doc . add_heading ( \"Results\" , level = 1 ) if self . summary : doc . add_heading ( \"Summary\" , level = 2 ) doc . add_paragraph ( self . summary ) doc . add_paragraph () for method , sections in results . items (): doc . add_heading ( str ( method ), level = 2 ) # 1\ufe0f\u20e3 keys the caller explicitly asked for, in order \u2026 for key in order : if key in sections : self . _add_table ( doc , sections [ key ], pretty_names . get ( key , self . _title_case ( key )) ) # 2\ufe0f\u20e3 any leftover keys (excluding those already shown) \u2026 for key , data in sections . items (): if key in order : continue self . _add_table ( doc , data , pretty_names . get ( key , self . _title_case ( key ))) doc . add_paragraph () # blank line between methods @staticmethod def _title_case ( text : str ) -> str : \"\"\"Generic fallback like 'bootstrap_confidence_intervals' \u2192 'Bootstrap Confidence Intervals'.\"\"\" return text . replace ( \"_\" , \" \" ) . title () def create_docx_report ( self ) -> Document : \"\"\" Create a DOCX report that includes a heading, the results section, and any associated figures. \"\"\" doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : self . _add_results_to_docx ( doc , self . results ) self . _add_figures ( doc ) return doc create_docx_report () Create a DOCX report that includes a heading, the results section, and any associated figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def create_docx_report ( self ) -> Document : \"\"\" Create a DOCX report that includes a heading, the results section, and any associated figures. \"\"\" doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : self . _add_results_to_docx ( doc , self . results ) self . _add_figures ( doc ) return doc FastAPIDocxCreator Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file convert_markdown_to_docx_bytes ( generated_response , template_filepath = None ) Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 162 163 164 165 166 167 168 169 170 171 172 173 174 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file StreamlitDocxCreator Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 138 139 140 141 142 143 144 145 146 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures , summary = None ): super () . __init__ ( summary = summary , results = results , figures = figures )","title":"Docx Creator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.DocxCreator","text":"Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , summary = None , results = None , figures = None ): self . results = results self . figures = figures self . summary = summary def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" , ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_figures ( self , doc : Document ): # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) buf . seek ( 0 ) doc . add_picture ( buf , width = Inches ( 5 )) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : Mapping [ str , Mapping [ str , object ]], * , order : tuple [ str , ... ] | None = None , pretty_names : Mapping [ str , str ] | None = None , ) -> None : \"\"\" Insert *all* results into the DOCX, no hard-coded section names required. Parameters ---------- doc : python-docx Document The document to write into. results : dict[str, dict[str, Any]] Outer keys = method names. Inner keys = arbitrary section names whose values can be rendered by ``self._add_table``. order : tuple[str] | None, default None If given, section keys in this tuple are shown first and in exactly this order; any remaining keys follow in their natural order. pretty_names : dict[str, str] | None, default None Optional mapping that converts raw section keys into nicer titles. Anything not listed falls back to `title_case(key)`. \"\"\" pretty_names = pretty_names or {} order = order or () doc . add_heading ( \"Results\" , level = 1 ) if self . summary : doc . add_heading ( \"Summary\" , level = 2 ) doc . add_paragraph ( self . summary ) doc . add_paragraph () for method , sections in results . items (): doc . add_heading ( str ( method ), level = 2 ) # 1\ufe0f\u20e3 keys the caller explicitly asked for, in order \u2026 for key in order : if key in sections : self . _add_table ( doc , sections [ key ], pretty_names . get ( key , self . _title_case ( key )) ) # 2\ufe0f\u20e3 any leftover keys (excluding those already shown) \u2026 for key , data in sections . items (): if key in order : continue self . _add_table ( doc , data , pretty_names . get ( key , self . _title_case ( key ))) doc . add_paragraph () # blank line between methods @staticmethod def _title_case ( text : str ) -> str : \"\"\"Generic fallback like 'bootstrap_confidence_intervals' \u2192 'Bootstrap Confidence Intervals'.\"\"\" return text . replace ( \"_\" , \" \" ) . title () def create_docx_report ( self ) -> Document : \"\"\" Create a DOCX report that includes a heading, the results section, and any associated figures. \"\"\" doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : self . _add_results_to_docx ( doc , self . results ) self . _add_figures ( doc ) return doc","title":"DocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.DocxCreator.create_docx_report","text":"Create a DOCX report that includes a heading, the results section, and any associated figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def create_docx_report ( self ) -> Document : \"\"\" Create a DOCX report that includes a heading, the results section, and any associated figures. \"\"\" doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : self . _add_results_to_docx ( doc , self . results ) self . _add_figures ( doc ) return doc","title":"create_docx_report"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"FastAPIDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator.convert_markdown_to_docx_bytes","text":"Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 162 163 164 165 166 167 168 169 170 171 172 173 174 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"convert_markdown_to_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.StreamlitDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 138 139 140 141 142 143 144 145 146 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures , summary = None ): super () . __init__ ( summary = summary , results = results , figures = figures )","title":"StreamlitDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/file_config.html","text":"","title":"File Config"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html","text":"create_base64_file_validator ( * allowed_mime_types ) Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file create_file_validator ( * allowed_mime_types ) Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file file_to_base64 ( filepath ) Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" ) ingest_docx ( file ) async The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here ingest_docx_bytes ( content ) The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here markdown_to_docx_temporary_file ( content , template_location = None ) The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path validate_date ( date_str = Query ( ... , description = 'The start date in YYYY-MM-DD format' )) Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"File Handling"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_base64_file_validator","text":"Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file","title":"create_base64_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_file_validator","text":"Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file","title":"create_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.file_to_base64","text":"Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" )","title":"file_to_base64"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx","text":"The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx_bytes","text":"The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.markdown_to_docx_temporary_file","text":"The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path","title":"markdown_to_docx_temporary_file"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.validate_date","text":"Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"validate_date"},{"location":"llm_utils/aiweb_common/file_operations/text_format.html","text":"","title":"Text Format"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html","text":"BytesToDocx Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx process_file_bytes ( file , extension = '.docx' ) The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx FastAPIUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e process_file_bytes ( file , extension ) Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) read_and_validate_file ( encoded_file , extension ) The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e StreamlitUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = ( file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] ) self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files , ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None __init__ ( file = None , message = 'Please upload a file' , file_types = None , accept_multiple_files = False , document_analysis_client = None ) Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = ( file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] ) self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client process_upload () If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files , ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) upload_file () Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 86 87 88 89 90 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"Upload Manager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx","text":"Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"BytesToDocx"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx.process_file_bytes","text":"The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"FastAPIUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.process_file_bytes","text":"Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" )","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.read_and_validate_file","text":"The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"read_and_validate_file"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = ( file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] ) self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files , ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None","title":"StreamlitUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.__init__","text":"Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = ( file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] ) self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client","title":"__init__"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.process_upload","text":"If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files , ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension )","title":"process_upload"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.upload_file","text":"Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 86 87 88 89 90 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"upload_file"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html","text":"AugmentedResponseHandler Bases: ResponseHandler This Python class AugmentedResponseHandler extends ResponseHandler and implements methods for processing input and generating responses using a language model service. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class AugmentedResponseHandler ( ResponseHandler ): \"\"\" This Python class `AugmentedResponseHandler` extends `ResponseHandler` and implements methods for processing input and generating responses using a language model service. \"\"\" def __init__ ( self , llm_interface ): super () . __init__ ( llm_interface ) def generate_response ( self , assembled_prompt ): \"\"\" The function `generate_response` calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the `generate_response` method is designed to generate a response using a language model. The `assembled_prompt` parameter likely contains the input prompt or context that will be used to generate the response :return: The `generate_response` method returns the response generated by calling the `generate_langchain_response` method of the `aug_service` object with the `assembled_prompt` as an argument. \"\"\" # Focus on the specifics of how to interact with the language model. # Implement the details of calling the LLM and handling the response. return self . aug_service . generate_langchain_response ( assembled_prompt ) generate_response ( assembled_prompt ) The function generate_response calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the generate_response method is designed to generate a response using a language model. The assembled_prompt parameter likely contains the input prompt or context that will be used to generate the response :return: The generate_response method returns the response generated by calling the generate_langchain_response method of the aug_service object with the assembled_prompt as an argument. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def generate_response ( self , assembled_prompt ): \"\"\" The function `generate_response` calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the `generate_response` method is designed to generate a response using a language model. The `assembled_prompt` parameter likely contains the input prompt or context that will be used to generate the response :return: The `generate_response` method returns the response generated by calling the `generate_langchain_response` method of the `aug_service` object with the `assembled_prompt` as an argument. \"\"\" # Focus on the specifics of how to interact with the language model. # Implement the details of calling the LLM and handling the response. return self . aug_service . generate_langchain_response ( assembled_prompt ) RAGResponseHandler Bases: AugmentedResponseHandler The RAGResponseHandler class extends AugmentedResponseHandler and initializes RAGServicer object with specified interfaces and a vector store. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 33 34 35 36 37 38 39 40 41 class RAGResponseHandler ( AugmentedResponseHandler ): \"\"\" The `RAGResponseHandler` class extends `AugmentedResponseHandler` and initializes `RAGServicer` object with specified interfaces and a vector store. \"\"\" def __init__ ( self , llm_interface , embedding_interface , vectorstore ): self . aug_service = RAGServicer ( llm_interface , embedding_interface , vectorstore ) super () . __init__ ( llm_interface ) SearchResponseHandler Bases: AugmentedResponseHandler The SearchResponseHandler class extends AugmentedResponseHandler and initializes a SearchServicer object for handling search responses. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 44 45 46 47 48 49 50 51 52 class SearchResponseHandler ( AugmentedResponseHandler ): \"\"\" The `SearchResponseHandler` class extends `AugmentedResponseHandler` and initializes a `SearchServicer` object for handling search responses. \"\"\" def __init__ ( self , llm_interface , searchable ): self . aug_service = SearchServicer ( llm_interface , searchable ) super () . __init__ ( llm_interface )","title":"AugmentedResponse"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html#aiweb_common.generate.AugmentedResponse.AugmentedResponseHandler","text":"Bases: ResponseHandler This Python class AugmentedResponseHandler extends ResponseHandler and implements methods for processing input and generating responses using a language model service. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class AugmentedResponseHandler ( ResponseHandler ): \"\"\" This Python class `AugmentedResponseHandler` extends `ResponseHandler` and implements methods for processing input and generating responses using a language model service. \"\"\" def __init__ ( self , llm_interface ): super () . __init__ ( llm_interface ) def generate_response ( self , assembled_prompt ): \"\"\" The function `generate_response` calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the `generate_response` method is designed to generate a response using a language model. The `assembled_prompt` parameter likely contains the input prompt or context that will be used to generate the response :return: The `generate_response` method returns the response generated by calling the `generate_langchain_response` method of the `aug_service` object with the `assembled_prompt` as an argument. \"\"\" # Focus on the specifics of how to interact with the language model. # Implement the details of calling the LLM and handling the response. return self . aug_service . generate_langchain_response ( assembled_prompt )","title":"AugmentedResponseHandler"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html#aiweb_common.generate.AugmentedResponse.AugmentedResponseHandler.generate_response","text":"The function generate_response calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the generate_response method is designed to generate a response using a language model. The assembled_prompt parameter likely contains the input prompt or context that will be used to generate the response :return: The generate_response method returns the response generated by calling the generate_langchain_response method of the aug_service object with the assembled_prompt as an argument. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def generate_response ( self , assembled_prompt ): \"\"\" The function `generate_response` calls a language model to generate a response based on a given prompt. :param assembled_prompt: It looks like the `generate_response` method is designed to generate a response using a language model. The `assembled_prompt` parameter likely contains the input prompt or context that will be used to generate the response :return: The `generate_response` method returns the response generated by calling the `generate_langchain_response` method of the `aug_service` object with the `assembled_prompt` as an argument. \"\"\" # Focus on the specifics of how to interact with the language model. # Implement the details of calling the LLM and handling the response. return self . aug_service . generate_langchain_response ( assembled_prompt )","title":"generate_response"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html#aiweb_common.generate.AugmentedResponse.RAGResponseHandler","text":"Bases: AugmentedResponseHandler The RAGResponseHandler class extends AugmentedResponseHandler and initializes RAGServicer object with specified interfaces and a vector store. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 33 34 35 36 37 38 39 40 41 class RAGResponseHandler ( AugmentedResponseHandler ): \"\"\" The `RAGResponseHandler` class extends `AugmentedResponseHandler` and initializes `RAGServicer` object with specified interfaces and a vector store. \"\"\" def __init__ ( self , llm_interface , embedding_interface , vectorstore ): self . aug_service = RAGServicer ( llm_interface , embedding_interface , vectorstore ) super () . __init__ ( llm_interface )","title":"RAGResponseHandler"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html#aiweb_common.generate.AugmentedResponse.SearchResponseHandler","text":"Bases: AugmentedResponseHandler The SearchResponseHandler class extends AugmentedResponseHandler and initializes a SearchServicer object for handling search responses. Source code in llm_utils/aiweb_common/generate/AugmentedResponse.py 44 45 46 47 48 49 50 51 52 class SearchResponseHandler ( AugmentedResponseHandler ): \"\"\" The `SearchResponseHandler` class extends `AugmentedResponseHandler` and initializes a `SearchServicer` object for handling search responses. \"\"\" def __init__ ( self , llm_interface , searchable ): self . aug_service = SearchServicer ( llm_interface , searchable ) super () . __init__ ( llm_interface )","title":"SearchResponseHandler"},{"location":"llm_utils/aiweb_common/generate/AugmentedServicer.html","text":"","title":"AugmentedServicer"},{"location":"llm_utils/aiweb_common/generate/ChatResponse.html","text":"ChatResponseHandler Bases: ResponseHandler Source code in llm_utils/aiweb_common/generate/ChatResponse.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChatResponseHandler ( ResponseHandler ): def __init__ ( self , llm_interface , prompt ): super () . __init__ ( llm_interface ) print ( \"initializing chat servicer\" ) self . chat_service = ChatServicer ( self . llm_interface , prompt ) def generate_response ( self , messages ): \"\"\" Generate a chat response using the language model and return a ChatResponse object. Args: messages (List[Message]): The chat history/messages. Returns: ChatResponse: The response object containing the AI's reply as a Message. \"\"\" from aiweb_common.generate.ChatSchemas import ChatResponse , Message # Get the raw response and metadata from the chat service response_content , response_meta = self . chat_service . generate_langchain_response ( messages ) # If the response includes an image update, encode it in Message.content as JSON # Convention: If response_content is a dict with 'text' and 'image_update', encode as JSON string import json if isinstance ( response_content , dict ) and \"text\" in response_content : # Example: {\"text\": \"...\", \"image_update\": {...}} content = json . dumps ( response_content ) else : content = response_content # Compose the Message object (role: ai) message = Message ( role = \"ai\" , content = content ) return ChatResponse ( response = message ) def update_history ( self , message , conversation_history ): return self . chat_service . update_history ( message , conversation_history ) generate_response ( messages ) Generate a chat response using the language model and return a ChatResponse object. Parameters: messages ( List [ Message ] ) \u2013 The chat history/messages. Returns: ChatResponse \u2013 The response object containing the AI's reply as a Message. Source code in llm_utils/aiweb_common/generate/ChatResponse.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def generate_response ( self , messages ): \"\"\" Generate a chat response using the language model and return a ChatResponse object. Args: messages (List[Message]): The chat history/messages. Returns: ChatResponse: The response object containing the AI's reply as a Message. \"\"\" from aiweb_common.generate.ChatSchemas import ChatResponse , Message # Get the raw response and metadata from the chat service response_content , response_meta = self . chat_service . generate_langchain_response ( messages ) # If the response includes an image update, encode it in Message.content as JSON # Convention: If response_content is a dict with 'text' and 'image_update', encode as JSON string import json if isinstance ( response_content , dict ) and \"text\" in response_content : # Example: {\"text\": \"...\", \"image_update\": {...}} content = json . dumps ( response_content ) else : content = response_content # Compose the Message object (role: ai) message = Message ( role = \"ai\" , content = content ) return ChatResponse ( response = message )","title":"ChatResponse"},{"location":"llm_utils/aiweb_common/generate/ChatResponse.html#aiweb_common.generate.ChatResponse.ChatResponseHandler","text":"Bases: ResponseHandler Source code in llm_utils/aiweb_common/generate/ChatResponse.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChatResponseHandler ( ResponseHandler ): def __init__ ( self , llm_interface , prompt ): super () . __init__ ( llm_interface ) print ( \"initializing chat servicer\" ) self . chat_service = ChatServicer ( self . llm_interface , prompt ) def generate_response ( self , messages ): \"\"\" Generate a chat response using the language model and return a ChatResponse object. Args: messages (List[Message]): The chat history/messages. Returns: ChatResponse: The response object containing the AI's reply as a Message. \"\"\" from aiweb_common.generate.ChatSchemas import ChatResponse , Message # Get the raw response and metadata from the chat service response_content , response_meta = self . chat_service . generate_langchain_response ( messages ) # If the response includes an image update, encode it in Message.content as JSON # Convention: If response_content is a dict with 'text' and 'image_update', encode as JSON string import json if isinstance ( response_content , dict ) and \"text\" in response_content : # Example: {\"text\": \"...\", \"image_update\": {...}} content = json . dumps ( response_content ) else : content = response_content # Compose the Message object (role: ai) message = Message ( role = \"ai\" , content = content ) return ChatResponse ( response = message ) def update_history ( self , message , conversation_history ): return self . chat_service . update_history ( message , conversation_history )","title":"ChatResponseHandler"},{"location":"llm_utils/aiweb_common/generate/ChatResponse.html#aiweb_common.generate.ChatResponse.ChatResponseHandler.generate_response","text":"Generate a chat response using the language model and return a ChatResponse object. Parameters: messages ( List [ Message ] ) \u2013 The chat history/messages. Returns: ChatResponse \u2013 The response object containing the AI's reply as a Message. Source code in llm_utils/aiweb_common/generate/ChatResponse.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def generate_response ( self , messages ): \"\"\" Generate a chat response using the language model and return a ChatResponse object. Args: messages (List[Message]): The chat history/messages. Returns: ChatResponse: The response object containing the AI's reply as a Message. \"\"\" from aiweb_common.generate.ChatSchemas import ChatResponse , Message # Get the raw response and metadata from the chat service response_content , response_meta = self . chat_service . generate_langchain_response ( messages ) # If the response includes an image update, encode it in Message.content as JSON # Convention: If response_content is a dict with 'text' and 'image_update', encode as JSON string import json if isinstance ( response_content , dict ) and \"text\" in response_content : # Example: {\"text\": \"...\", \"image_update\": {...}} content = json . dumps ( response_content ) else : content = response_content # Compose the Message object (role: ai) message = Message ( role = \"ai\" , content = content ) return ChatResponse ( response = message )","title":"generate_response"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html","text":"Message Bases: BaseModel Source code in llm_utils/aiweb_common/generate/ChatSchemas.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Message ( BaseModel ): role : Role = Field ( example = Role . human ) content : str # May be plain text or a JSON string encoding image update triggers time : datetime = Field ( default_factory = lambda : datetime . now ( pytz . timezone ( \"US/Central\" )), description = \"The time the message was created\" , example = datetime . now ( pytz . timezone ( \"US/Central\" )) . isoformat (), # Example in Central Time ) def __init__ ( self , ** data ): super () . __init__ ( ** data ) # Convert time to Central Time if it's not already if self . time . tzinfo is None : self . time = pytz . timezone ( \"US/Central\" ) . localize ( self . time ) else : self . time = self . time . astimezone ( pytz . timezone ( \"US/Central\" )) def get_image_update ( self ): \"\"\" If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None. Returns ------- dict or None The image update trigger dict if present, else None. Example ------- >>> msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') >>> msg.get_image_update() {'url': 'img.png'} \"\"\" import json try : obj = json . loads ( self . content ) if isinstance ( obj , dict ) and \"image_update\" in obj : return obj [ \"image_update\" ] except Exception : pass return None class Config : json_encoders = { datetime : lambda v : v . astimezone ( pytz . timezone ( \"US/Central\" )) . isoformat ()} get_image_update () If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None. Returns dict or None The image update trigger dict if present, else None. Example msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') msg.get_image_update() {'url': 'img.png'} Source code in llm_utils/aiweb_common/generate/ChatSchemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def get_image_update ( self ): \"\"\" If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None. Returns ------- dict or None The image update trigger dict if present, else None. Example ------- >>> msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') >>> msg.get_image_update() {'url': 'img.png'} \"\"\" import json try : obj = json . loads ( self . content ) if isinstance ( obj , dict ) and \"image_update\" in obj : return obj [ \"image_update\" ] except Exception : pass return None","title":"ChatSchemas"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html#aiweb_common.generate.ChatSchemas.Message","text":"Bases: BaseModel Source code in llm_utils/aiweb_common/generate/ChatSchemas.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Message ( BaseModel ): role : Role = Field ( example = Role . human ) content : str # May be plain text or a JSON string encoding image update triggers time : datetime = Field ( default_factory = lambda : datetime . now ( pytz . timezone ( \"US/Central\" )), description = \"The time the message was created\" , example = datetime . now ( pytz . timezone ( \"US/Central\" )) . isoformat (), # Example in Central Time ) def __init__ ( self , ** data ): super () . __init__ ( ** data ) # Convert time to Central Time if it's not already if self . time . tzinfo is None : self . time = pytz . timezone ( \"US/Central\" ) . localize ( self . time ) else : self . time = self . time . astimezone ( pytz . timezone ( \"US/Central\" )) def get_image_update ( self ): \"\"\" If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None. Returns ------- dict or None The image update trigger dict if present, else None. Example ------- >>> msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') >>> msg.get_image_update() {'url': 'img.png'} \"\"\" import json try : obj = json . loads ( self . content ) if isinstance ( obj , dict ) and \"image_update\" in obj : return obj [ \"image_update\" ] except Exception : pass return None class Config : json_encoders = { datetime : lambda v : v . astimezone ( pytz . timezone ( \"US/Central\" )) . isoformat ()}","title":"Message"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html#aiweb_common.generate.ChatSchemas.Message.get_image_update","text":"If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None.","title":"get_image_update"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html#aiweb_common.generate.ChatSchemas.Message.get_image_update--returns","text":"dict or None The image update trigger dict if present, else None.","title":"Returns"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html#aiweb_common.generate.ChatSchemas.Message.get_image_update--example","text":"msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') msg.get_image_update() {'url': 'img.png'} Source code in llm_utils/aiweb_common/generate/ChatSchemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def get_image_update ( self ): \"\"\" If content encodes an image update trigger (as JSON), return the image update dict. Otherwise, return None. Returns ------- dict or None The image update trigger dict if present, else None. Example ------- >>> msg = Message(content='{\"text\": \"Here is your image\", \"image_update\": {\"url\": \"img.png\"}}') >>> msg.get_image_update() {'url': 'img.png'} \"\"\" import json try : obj = json . loads ( self . content ) if isinstance ( obj , dict ) and \"image_update\" in obj : return obj [ \"image_update\" ] except Exception : pass return None","title":"Example"},{"location":"llm_utils/aiweb_common/generate/ChatServicer.html","text":"ChatServicer Bases: QueryInterface Source code in llm_utils/aiweb_common/generate/ChatServicer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ChatServicer ( QueryInterface ): def __init__ ( self , language_model_interface , prompt ): super () . __init__ ( language_model_interface ) assembled_system_chat_template = self . preparer . assemble_chat_template ( prompt = prompt ) self . assembled_system_chat_template = assembled_system_chat_template def generate_langchain_response ( self , messages ): \"\"\" Generate a response from the language model, supporting image update triggers. Args: messages (List[Message]): The chat history/messages. Returns: Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. \"\"\" chain = self . assembled_system_chat_template | self . language_model_interface with get_openai_callback () as response_meta : response = chain . invoke ({ \"messages\" : messages }) # If the response contains an image update, encode as dict # Convention: If response has 'image_update' attribute, return dict if hasattr ( response , \"image_update\" ) and response . image_update is not None : return { \"text\" : response . content , \"image_update\" : response . image_update }, response_meta return response . content , response_meta def update_history ( self , message , chat_history ): if message . role == \"ai\" : chat_history . append ( AIMessage ( content = message . content )) elif message . role == \"human\" : chat_history . append ( HumanMessage ( content = message . content )) return chat_history generate_langchain_response ( messages ) Generate a response from the language model, supporting image update triggers. Parameters: messages ( List [ Message ] ) \u2013 The chat history/messages. Returns: \u2013 Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. Source code in llm_utils/aiweb_common/generate/ChatServicer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def generate_langchain_response ( self , messages ): \"\"\" Generate a response from the language model, supporting image update triggers. Args: messages (List[Message]): The chat history/messages. Returns: Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. \"\"\" chain = self . assembled_system_chat_template | self . language_model_interface with get_openai_callback () as response_meta : response = chain . invoke ({ \"messages\" : messages }) # If the response contains an image update, encode as dict # Convention: If response has 'image_update' attribute, return dict if hasattr ( response , \"image_update\" ) and response . image_update is not None : return { \"text\" : response . content , \"image_update\" : response . image_update }, response_meta return response . content , response_meta","title":"ChatServicer"},{"location":"llm_utils/aiweb_common/generate/ChatServicer.html#aiweb_common.generate.ChatServicer.ChatServicer","text":"Bases: QueryInterface Source code in llm_utils/aiweb_common/generate/ChatServicer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ChatServicer ( QueryInterface ): def __init__ ( self , language_model_interface , prompt ): super () . __init__ ( language_model_interface ) assembled_system_chat_template = self . preparer . assemble_chat_template ( prompt = prompt ) self . assembled_system_chat_template = assembled_system_chat_template def generate_langchain_response ( self , messages ): \"\"\" Generate a response from the language model, supporting image update triggers. Args: messages (List[Message]): The chat history/messages. Returns: Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. \"\"\" chain = self . assembled_system_chat_template | self . language_model_interface with get_openai_callback () as response_meta : response = chain . invoke ({ \"messages\" : messages }) # If the response contains an image update, encode as dict # Convention: If response has 'image_update' attribute, return dict if hasattr ( response , \"image_update\" ) and response . image_update is not None : return { \"text\" : response . content , \"image_update\" : response . image_update }, response_meta return response . content , response_meta def update_history ( self , message , chat_history ): if message . role == \"ai\" : chat_history . append ( AIMessage ( content = message . content )) elif message . role == \"human\" : chat_history . append ( HumanMessage ( content = message . content )) return chat_history","title":"ChatServicer"},{"location":"llm_utils/aiweb_common/generate/ChatServicer.html#aiweb_common.generate.ChatServicer.ChatServicer.generate_langchain_response","text":"Generate a response from the language model, supporting image update triggers. Parameters: messages ( List [ Message ] ) \u2013 The chat history/messages. Returns: \u2013 Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. Source code in llm_utils/aiweb_common/generate/ChatServicer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def generate_langchain_response ( self , messages ): \"\"\" Generate a response from the language model, supporting image update triggers. Args: messages (List[Message]): The chat history/messages. Returns: Tuple[str|dict, Any]: The response content (str or dict if image update) and metadata. \"\"\" chain = self . assembled_system_chat_template | self . language_model_interface with get_openai_callback () as response_meta : response = chain . invoke ({ \"messages\" : messages }) # If the response contains an image update, encode as dict # Convention: If response has 'image_update' attribute, return dict if hasattr ( response , \"image_update\" ) and response . image_update is not None : return { \"text\" : response . content , \"image_update\" : response . image_update }, response_meta return response . content , response_meta","title":"generate_langchain_response"},{"location":"llm_utils/aiweb_common/generate/PromptAssembler.html","text":"","title":"PromptAssembler"},{"location":"llm_utils/aiweb_common/generate/PromptyHandler.html","text":"","title":"PromptyHandler"},{"location":"llm_utils/aiweb_common/generate/PromptyResponse.html","text":"","title":"PromptyResponse"},{"location":"llm_utils/aiweb_common/generate/PromptyResponseHandler.html","text":"","title":"PromptyResponse"},{"location":"llm_utils/aiweb_common/generate/PromptyServicer.html","text":"","title":"PromptyServicer"},{"location":"llm_utils/aiweb_common/generate/QueryInterface.html","text":"","title":"QueryInterface"},{"location":"llm_utils/aiweb_common/generate/Response.html","text":"","title":"Response"},{"location":"llm_utils/aiweb_common/generate/SingleResponse.html","text":"","title":"SingleResponse"},{"location":"llm_utils/aiweb_common/generate/SingleResponseServicer.html","text":"","title":"SingleResponseServicer"},{"location":"llm_utils/aiweb_common/resource/NIHRePORTERInterface.html","text":"","title":"NIH RePORTER Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html","text":"PubMedInterface Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] fetch_article_details ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None fetch_article_details_xml ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] search_pubmed_articles ( query ) The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"PubMed Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface","text":"Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"PubMedInterface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None","title":"fetch_article_details"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details_xml","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"fetch_article_details_xml"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.search_pubmed_articles","text":"The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = ( f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" ) if attempt < self . max_retries : wait_message = ( f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" ) print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"search_pubmed_articles"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html","text":"PubMedQueryGenerator Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query assembled_prompt = self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content generate_search_string ( loop_n = 0 , last_query = '' ) The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query assembled_prompt = self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMed Query"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator","text":"Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query assembled_prompt = self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMedQueryGenerator"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator.generate_search_string","text":"The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query assembled_prompt = self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"generate_search_string"},{"location":"llm_utils/aiweb_common/resource/default_resource_config.html","text":"","title":"Default Resource Config"},{"location":"llm_utils/aiweb_common/streamlit/BYOKLogin.html","text":"","title":"Bring Your Own Key (BYOK)"},{"location":"llm_utils/aiweb_common/streamlit/streamlit_common.html","text":"","title":"Streamlit Common"},{"location":"tests/index.html","text":"This folder if for software unit testing","title":"Tests"},{"location":"tests/index.html#this-folder-if-for-software-unit-testing","text":"","title":"This folder if for software unit testing"}]}